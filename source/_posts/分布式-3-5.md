---
title: 分布式缓存--热点数据的拯救者
date: 2020-4-28 09:18:59
tags:
 - 分布式
 - 缓存
categories:
 - 分布式
---

缓存是分布式系统中的重要组件，主要解决高并发，大数据场景下，热点数据访问的性能问题。提供高性能的数据快速访问。

<!--more-->

### 原理

1. 将数据写入/读取速度更快的存储（设备）；
2. 将数据缓存到离应用最近的位置；
3. 将数据缓存到离用户最近的位置。

#### 缓存分类

在分布式系统中，缓存的应用非常广泛，从部署角度有以下几个方面的缓存应用。

1. CDN缓存；
2. 反向代理缓存；
3. 分布式Cache；
4. 本地应用缓存；

#### 缓存媒介

常用中间件：Varnish，Ngnix，Squid，Memcache，Redis，Ehcache等；

缓存的内容：文件，数据，对象；

缓存的介质：CPU，内存（本地，分布式），磁盘（本地，分布式）

#### 缓存设计

缓存设计需要解决以下几个问题：

1. 缓存什么？

   哪些数据需要缓存：1.热点数据；2.静态资源；

2. 缓存的位置？

   CDN，反向代理，分布式缓存服务器，本机（内存，硬盘）

3. 如何缓存的问题？

   - 过期策略

     1. 固定时间：比如指定缓存的时间是30分钟；

     2. 相对时间：比如最近10分钟内没有访问的数据；

   - 同步机制
     1. 实时写入；（推）
     2. 异步刷新；（推拉）

#### 使用场景

![MEsJfg.png](https://s2.ax1x.com/2019/11/08/MEsJfg.png)

#### 三要素

命中率、缓存更新策略、缓存最大数据量。

1. 命中率

   通过命中率衡量缓存机制的好坏和效率。命中率指请求缓存次数和缓存返回正确结果次数的比例。比例越高，证明缓存的使用率越高。需要注意的是，如果数据频繁更新，就需要考虑缓存的合理性。因为缓存更新会使命中率大大降低，当命中率比较低，缓存不仅不能提高效率，反而可能会造成负面影响。

2. 缓存更新策略
   缓存的容器都有大小限制，我们在对数据进行缓存的过程中，最后达到缓存可以容纳的极限；或者我们对原有的数据进行了操作，操作包括新增、修改、删除等；这个时候就必须考虑缓存内容更新问题。缓存更新策略归纳为以下几种：

   - FIFO：first in first out。最先进入缓存的数据在缓存空间不够情况下会被首先清空出去；
   - LFU：less Frequently Used。最少使用的元素会被清理丢。这要求缓存的元素有hit属性，在缓存空间不够的情况下，hit值最小的将会被清除缓存；
   - FRU：least Recently Used。最近最少使用的元素被清理。缓存的元素有一个时间戳，当缓存容量满了，而又需要腾出缓存的地方，现有的缓存元素中时间戳离当前时间最远的元素将被清空出缓存。

3. 缓存的最大数据量

   缓存最大数据量是在缓存中能处理元素的最大个数或所能使用的最大存储空间。通常各种缓存机制都会对缓存最大数据量进行限制，可以是固定大小的存储空间、集合个数，或者由操作系统所能分配和处理的存储空间决定。

   例如mysql的Query cache缓存最大数据量由query_cache_size参数决定，可以修改；而基于内存的key-value实施方案Memcached，其缓存最大数据量可使用内存由操作系统决定，默认为64MB，每次最大可申请内存为2M.

   超过缓存机制所允许的最大数据量系统会进行相应处理，一般有四种处理方式：

   - 停止缓存服务，所有缓存数据被清空；

   - 拒绝写入，不再对缓存数据进行更新；

   - 根据缓存更新策略清空旧数据；

   - 在方式3基础上，将淘汰的数据备份，腾出新的空间。

   在实际的应用中，通常以方式3和4最为常见。

### 缓存分类

#### CDN缓存

CDN主要解决将数据缓存到离用户最近的位置，一般缓存静态资源文件（页面，脚本，图片，视频，文件等）。国内网络异常复杂，跨运营商的网络访问会很慢。为了解决跨运营商或各地用户访问问题，可以在重要的城市，部署CDN应用。使用户就近获取所需内容，降低网络拥塞，提高用户访问响应速度和命中率。

**CND原理**

CDN的基本原理是广泛采用各种缓存服务器，将这些缓存服务器分布到用户访问相对集中的地区或网络中，在用户访问网站时，利用全局负载技术将用户的访问指向距离最近的工作正常的缓存服务器上，由缓存服务器直接响应用户请求。

1. 未部署CDN应用前

   ![YDB4D1.png](https://s1.ax1x.com/2020/05/14/YDB4D1.png)

   网络请求路径：

   请求：本机网络（局域网）——》运营商网络——》应用服务器机房

   响应：应用服务器机房——》运营商网络——》本机网络（局域网）

   在不考虑复杂网络的情况下，从请求到响应需要经过3个节点，6个步骤完成一次用户访问操作。

2. 部署CDN应用后

   ![YDBvDI.png](https://s1.ax1x.com/2020/05/14/YDBvDI.png)

   网络路径：

   请求：本机网络（局域网）——》运营商网络

   响应：运营商网络——》本机网络（局域网）

   在不考虑复杂网络的情况下，从请求到响应需要经过2个节点，2个步骤完成一次用户访问操作。

   与不部署CDN服务相比，减少了1个节点，4个步骤的访问。极大的提高的系统的响应速度。

**CDN优缺点**

1. 优点（摘自百度百科）

   1. 本地Cache加速：提升访问速度，尤其含有大量图片和静态页面站点；
   2. 镜像服务：消除了不同运营商之间互联的瓶颈造成的影响，实现了跨运营商的网络加速，保证不同网络中的用户都能得到良好的访问质量；
   3. 远程加速：远程访问用户根据DNS负载均衡技术智能自动选择Cache服务器，选择最快的Cache服务器，加快远程访问的速度；
   4. 带宽优化：自动生成服务器的远程Mirror（镜像）cache服务器，远程用户访问时从cache服务器上读取数据，减少远程访问的带宽、分担网络流量、减轻原站点WEB服务器负载等功能。
   5. 集群抗攻击：广泛分布的CDN节点加上节点之间的智能冗余机制，可以有效地预防黑客入侵以及降低各种D.D.o.S攻击对网站的影响，同时保证较好的服务质量。

2. 缺点

   1. 动态资源缓存，需要注意实时性；

      解决：主要缓存静态资源，动态资源建立多级缓存或准实时同步；

   2. 如何保证数据的一致性和实时性需要权衡考虑

      解决：

      1. 设置缓存失效时间（1个小时，最终一致性）；
      2. 数据版本号；

#### 反向代理缓存

反向代理是指在网站服务器机房部署代理服务器，实现负载均衡，数据缓存，安全控制等功能。

**缓存原理**

反向代理位于应用服务器机房，处理所有对WEB服务器的请求。如果用户请求的页面在代理服务器上有缓冲的话，代理服务器直接将缓冲内容发送给用户。如果没有缓冲则先向WEB服务器发出请求，取回数据，本地缓存后再发送给用户。通过降低向WEB服务器的请求数，从而降低了WEB服务器的负载。

![YDDBxH.png](https://s1.ax1x.com/2020/05/14/YDDBxH.png)

反向代理一般缓存静态资源，动态资源转发到应用服务器处理。常用的缓存应用服务器有Varnish，Ngnix，Squid。简单比较如下：

1. varnish和squid是专业的cache服务，nginx需要第三方模块支持；
2. Varnish采用内存型缓存，避免了频繁在内存、磁盘中交换文件，性能比Squid高；
3. Varnish由于是内存cache，所以对小文件如css,js,小图片啥的支持很棒，后端的持久化缓存可以采用的是Squid或ATS；
4. Squid功能全而大，适合于各种静态的文件缓存，一般会在前端挂一个HAProxy或nginx做负载均衡跑多个实例；
5. Nginx采用第三方模块ncache做的缓冲，性能基本达到varnish，一般作为反向代理使用，可以实现简单的缓存。

#### 分布式缓存

CDN,反向代理缓存，主要解决静态文件，或用户请求资源的缓存，数据源一般为静态文件或动态生成的文件（有缓存头标识）。

**分布式缓存，主要指缓存用户经常访问的数据，数据源为数据库。一般起到热点数据访问和减轻数据库压力的作用。**

目前分布式缓存设计，在大型网站架构中是必备的架构要素。常用的中间件有Memcache，Redis。

##### Memcache

Memcache是一个高性能，分布式内存对象缓存系统，通过在**内存**里维护一个统一的巨大的**hash**表，它能够用来存储各种格式的数据，包括图像、视频、文件以及数据库检索的结果等。***简单的说就是将数据调用到内存中，然后从内存中读取，从而大大提高读取速度。\***

Memcache特性：

1. 使用物理内存作为缓存区，可独立运行在服务器上。每个进程最大2G，如果想缓存更多的数据，可以开辟更多的memcache进程（不同端口）或者使用分布式memcache进行缓存，将数据缓存到不同的物理机或者虚拟机上。
2. 使用***key-value***的方式来存储数据，这是一种单索引的结构化数据组织形式，可使数据项查询时间复杂度为O(1)。
3. 协议简单：基于文本行的协议，直接通过telnet在memcached服务器上可进行存取数据操作，简单，方便多种缓存参考此协议；
4. 基于libevent高性能通信：Libevent是一套利用C开发的程序库，它将BSD系统的kqueue,Linux系统的epoll等事件处理功能封装成一个接口，与传统的select相比，提高了性能。
5. 内置的内存管理方式：**所有数据都保存在内存中，存取数据比硬盘快，当内存满后，通过LRU算法自动删除不使用的缓存，但没有考虑数据的容灾问题，重启服务，所有数据会丢失**。
6. 分布式：**各个memcached服务器之间互不通信，各自独立存取数据，不共享任何信息。服务器并不具有分布式功能，分布式部署取决于memcache客户端**。
7. 缓存策略：Memcached的缓存策略是LRU（最近最少使用）到期失效策略。在memcached内存储数据项时，可以指定它在缓存的失效时间，默认为永久。当memcached服务器用完分配的内时，失效的数据被首先替换，然后也是最近未使用的数据。在LRU中，memcached使用的是一种Lazy Expiration策略，自己不会监控存入的key/vlue对是否过期，而是在获取key值时查看记录的时间戳，检查key/value对空间是否过期，这样可减轻服务器的负载。

**Memcache工作原理**

![YDDXJU.png](https://s1.ax1x.com/2020/05/14/YDDXJU.png)

MemCache的工作流程如下：

1. 先检查客户端的请求数据是否在memcached中，如有，直接把请求数据返回，不再对数据库进行任何操作；
2. 如果请求的数据不在memcached中，就去查数据库，把从数据库中获取的数据返回给**客户端**，同时把数据缓存一份到memcached中（memcached客户端不负责，需要程序实现）；
3. 每次更新数据库的同时更新memcached中的数据，保证一致性；
4. 当分配给memcached内存空间用完之后，会使用LRU（Least Recently Used，最近最少使用）策略加上到期失效策略，失效数据首先被替换，然后再替换掉最近未使用的数据。

**Memcache集群**

memcached 虽然称为 “ 分布式 ” 缓存服务器，但服务器端并没有 “ 分布式 ” 功能。**每个服务器都是完全独立和隔离的服务**。 memcached 的分布式，是由客户端程序实现的。

当向memcached集群存入/取出key value时，memcached客户端程序根据一定的算法计算存入哪台服务器，然后再把key value值存到此服务器中。

存取数据分二步走，第一步，选择服务器，第二步存取数据。

![YDr8fS.png](https://s1.ax1x.com/2020/05/14/YDr8fS.png)

使用一致性Hash算法选择服务器

##### Redis

Redis 是一个开源（BSD许可）的，基于**内存**的，多数据结构存储系统。可以用作数据库、缓存和消息中间件。 支持多种类型的数据结构，如字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。内置了复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions）和不同级别的磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动分区（Cluster）提供高可用性（high availability）。

##### Memcache与Redis的比较

1. 数据结构：Memcache只支持key value存储方式，Redis支持更多的数据类型，比如Key value，hash，list，set，zset；
2. 多线程：Memcache支持多线程，redis支持单线程；CPU利用方面Memcache优于redis；
3. 持久化：Memcache不支持持久化，Redis支持持久化；
4. 内存利用率：memcache高，redis低（采用压缩的情况下比memcache高）；
5. 过期策略：memcache过期后，不删除缓存，会导致下次取数据数据的问题，Redis有专门线程，清除缓存数据；

#### 本地缓存

本地缓存是指应用内部的缓存，标准的分布式系统，一般有多级缓存构成。本地缓存是离应用最近的缓存，一般可以将数据缓存到硬盘或内存。

**硬盘缓存**

将数据缓存到硬盘到，读取时从硬盘读取。原理是直接读取本机文件，减少了网络传输消耗，比通过网络读取数据库速度更快。可以应用在对速度要求不是很高，但需要大量缓存存储的场景。

**内存缓存**

直接将数据存储到本机内存中，通过程序直接维护缓存对象，是访问速度最快的方式。

#### 缓存架构示例

![YDrW01.png](https://s1.ax1x.com/2020/05/14/YDrW01.png)

职责划分：

- CDN：存放HTML,CSS,JS等静态资源；
- 反向代理：动静分离，只缓存用户请求的静态资源；
- 分布式缓存：缓存数据库中的热点数据；
- 本地缓存：缓存应用字典等常用数据；

请求过程：

1. 浏览器向客户端发起请求，如果CDN有缓存则直接返回；
2. 如果CDN无缓存，则访问反向代理服务器；
3. 如果反向代理服务器有缓存则直接返回；
4. 如果反向代理服务器无缓存或动态请求，则访问应用服务器；
5. 应用服务器访问本地缓存；如果有缓存，则返回代理服务器，并缓存数据；（动态请求不缓存）
6. 如果本地缓存无数据，则读取分布式缓存；并返回应用服务器；应用服务器将数据缓存到本地缓存（部分）；
7. 如果分布式缓存无数据，则应用程序读取数据库数据，并放入分布式缓存；

### 数据一致性

缓存是在数据持久化之前的一个节点，主要是将热点数据放到离用户最近或访问速度更快的介质中，加快数据的访问，减小响应时间。

因为缓存属于持久化数据的一个副本，因此不可避免的会出现数据不一致问题。导致脏读或读不到数据的情况。数据不一致，一般是因为网络不稳定或节点故障导致。根据数据的操作顺序，主要有以下几种情况。

#### 场景介绍

1. 先写缓存，再写数据库

   ![YDs1BR.png](https://s1.ax1x.com/2020/05/14/YDs1BR.png)

   假如缓存写成功，但写数据库失败或响应延迟，则下次读取（并发读）缓存时，就出现脏读；

   同时有请求A和请求B进行更新操作，那么会出现

   （1）线程A更新了数据库

   （2）线程B更新了数据库

   （3）线程B更新了缓存

   （4）线程A更新了缓存

   这就出现请求A更新缓存应该比请求B更新缓存早才对，但是因为网络等原因，B却比A更早更新了缓存。这就导致了脏数据，因此不考虑。

2. 先删除缓存,再更新数据库
   该方案会导致不一致的原因是。同时有一个请求A进行更新操作，另一个请求B进行查询操作。那么会出现如下情形:

   （1）请求A进行写操作，删除缓存

   （2）请求B查询发现缓存不存在

   （3）请求B去数据库查询得到旧值

   （4）请求B将旧值写入缓存

   （5）请求A将新值写入数据库

   这种线程安全问题需要通过延时双删等方案解决

   大概的策略是：

   （1）先淘汰缓存

   （2）再写数据库（这两步和原来一样）

   （3）休眠x秒，再次淘汰缓存

   也不考虑

3. 先写数据库，再写缓存

   ![YDsDHI.png](https://s1.ax1x.com/2020/05/14/YDsDHI.png)

   假如写数据库成功，但写缓存失败，则下次读取（并发读）缓存时，则读不到数据；

   那么，是不是Cache Aside这个就不会有并发问题了？不是的，比如，一个是读操作，但是没有命中缓存，然后就到数据库中取数据，此时来了一个写操作，写完数据库后，让缓存失效，然后，之前的那个读操作再把老的数据放进去，所以，会造成脏数据。

   这个case理论上会出现，不过，实际上出现的概率可能非常低，因为这个条件需要发生在读缓存时缓存失效，而且并发着有一个写操作。而实际上数据库的写操作会比读操作慢得多，而且还要锁表，而读操作必需在写操作前进入数据库操作，而又要晚于写操作更新缓存，所有的这些条件都具备的概率基本并不大。

4. 缓存异步刷新

   指数据库操作和写缓存不在一个操作步骤中，比如在分布式场景下，无法做到同时写缓存或需要异步刷新（补救措施）时候。

   ![YDs28S.png](https://s1.ax1x.com/2020/05/14/YDs28S.png)

   此种情况，主要考虑数据写入和缓存刷新的时效性。比如多久内刷新缓存，不影响用户对数据的访问。

#### 解决方法

第一个场景：

***这个写缓存的方式，本身就是错误的，需要改为先写持久化介质，再写缓存的方式。***

第二个场景：

1. 根据写入缓存的响应来进行判断，如果缓存写入失败，则回滚数据库操作；此种方法增加了程序的复杂度，不建议采用；
2. 缓存使用时，假如读缓存失败，先读数据库，再回写缓存的方式实现。

第三个场景：

1. 首先确定，哪些数据适合此类场景；
2. 根据经验值确定合理的数据不一致时间，用户数据刷新的时间间隔；

**其他方法**

1. 超时：设置合理的超时时间；
2. 刷新：定时刷新一定范围内（根据时间，版本号）的数据；

以上是简化数据读写场景，实际中会分为：

1. 缓存与数据库之间的一致性；
2. 多级缓存之前的一致性；
3. 缓存副本之前的一致性。

#### 关于脏数据，如果需要强一致性

1. 可以通过数据库无论是读或写操作都是通过一个请求db connection连接完成（目的是串行），这样就需要修改连接池

2. 可以采用更新缓存而不是淘汰缓存，前提是更新的代价比较低

3. 可以先更新数据库再淘汰缓存，不过一般情况，淘汰缓存失败的可能性很小，可以以缓存处理100%不失败为前期。

4. 双淘汰法，即：淘汰缓存-更新数据库-淘汰缓存，可以尽量减少脏数据的留存时间

   以上实现起来，要么极短时间的不一致要么一致性代价比较高，实际项目我会这样处理，更新数据库的地方和读取的地方上同样key的分布式锁，这样就能保证，先操作（或读或写）数据的先获得结果，实际中这样的强一致需求比较少，参考思路即可。

   当然数据既然都缓存起来了，绝大部分都不要求强一致性，为了尽可能的缩短一致性的时间，可以如下处理：

6. 异步消息总线esb更新法，即：修改数据库往消息总线里发送一个消息，在接收端去处理这个消息更新缓存，缺点是有代码入侵

7. 异步binlog扫描更新法，增量的去扫描binlog中的修改记录，符合条件的更新缓存，相比消息总线法没有代码入侵

### 缓存高可用

业界有两种理论，第一套缓存就是缓存，临时存储数据的，不需要高可用。第二种缓存逐步演化为重要的存储介质，需要做高可用。

本人的看法是，缓存是否高可用，需要根据实际的场景而定。临界点是是否对后端的数据库造成影响。

具体的决策依据需要根据，集群的规模（数据，缓存），成本（服务器，运维），系统性能（并发量，吞吐量，响应时间）等方面综合评价。

#### 解决方法

缓存的高可用，一般通过分布式和复制实现。分布式实现数据的海量缓存，复制实现缓存数据节点的高可用。架构图如下：

![YDyi8O.png](https://s1.ax1x.com/2020/05/14/YDyi8O.png)

其中，分布式采用一致性Hash算法，复制采用异步复制。

#### 其他方法

1. 复制双写：缓存节点的复制，由异步改为双写，只有两份都写成功，才算成功。
2. 虚拟层：一致性Hash存在，假如其中一个HASH环不可用，数据会写入临近的环，当HASH可用时，数据又写入正常的HASH环，会导致数据偏移问题。这种情况，可以考虑在HASH环前面加一个虚拟层实现。
3. 多级缓存：比如一级使用本地缓存，二级采用分布式Cahce，三级采用分布式Cache+本地持久化；

方式很多，需要根据业务场景灵活选择。

### 缓存问题及解决方案

虽然使用缓存给系统带来了一定的质的提升，但同时也带来了一些需要注意的问题。

#### 缓存穿透

在高并发下,多线程同时查询同一个资源,如果缓存中没有这个资源,那么这些线程都会去数据库查找,对数据库造成极大压力,缓存失去存在的意义.打个比方,数据库是人,缓存是防弹衣,子弹是线程,本来防弹衣是防止子弹打到人身上的,但是当防弹衣里面没有防弹的物质时,子弹就会穿过它打到人身上. 

![MEyb5T.png](https://s2.ax1x.com/2019/11/08/MEyb5T.png)

缓存穿透是指**查询一个一定不存在的数据**，因为缓存中也无该数据的信息，则会直接去数据库层进行查询，从系统层面来看像是穿透了缓存层直接达到db，从而称为缓存穿透，没有了缓存层的保护，这种查询一定不存在的数据对系统来说可能是一种危险，如果有人恶意用这种一定不存在的数据来频繁请求系统，不，准确的说是攻击系统，请求都会到达数据库层导致db瘫痪从而引起系统故障。

**解决方案**

缓存穿透业内的解决方案已经比较成熟，主要常用的有以下几种：

- bloom filter：类似于哈希表的一种算法，用所有可能的查询条件生成一个bitmap，在进行数据库查询之前会使用这个bitmap进行过滤，如果不在其中则直接过滤，从而减轻数据库层面的压力。**guava中有实现BloomFilter算法**。
- 空值缓存：一种比较简单的解决办法，在第一次查询完不存在的数据后，将该key与对应的空值也放入缓存中，只不过设定为较短的失效时间，例如几分钟，这样则可以应对短时间的大量的该key攻击，设置为较短的失效时间是因为该值可能业务无关，存在意义不大，且该次的查询也未必是攻击者发起，无过久存储的必要，故可以早点失效。

#### 缓存雪崩

在普通的缓存系统中一般例如redis、memcache等中，我们会给缓存设置一个失效时间，但是如果所有的缓存的失效时间相同，那么在同一时间失效时，所有系统的请求都会发送到数据库层，db可能无法承受如此大的压力导致系统崩溃。

比如一个雪崩的简单过程：

1. redis集群大面积故障
2. 缓存失效，但依然大量请求访问缓存服务redis
3. redis大量失效后，大量请求转向到mysql数据库
4. mysql的调用量暴增，很快就扛不住了，甚至直接宕机
5. 由于大量的应用服务依赖mysql和redis的服务，这个时候很快会演变成各服务器集群的雪崩，网站彻底崩溃。

**解决方案**

- 线程互斥：只让一个线程构建缓存，其他线程等待构建缓存的线程执行完，重新从缓存获取数据才可以，每个时刻只有一个线程在执行请求，减轻了db的压力，但缺点也很明显，降低了系统的qps。
- 交错失效时间：这种方法时间比较简单粗暴，既然在同一时间失效会造成请求过多雪崩，那我们错开不同的失效时间即可从一定长度上避免这种问题，在缓存进行失效时间设置的时候，从某个适当的值域中随机一个时间作为失效时间即可。
- 合理评估数据库的负载压力；
- 对数据库进行过载保护或应用层限流；
- 多级缓存设计，缓存高可用；

#### 缓存并发

这里的并发指的是多个redis的client同时set key引起的并发问题。其实redis自身就是单线程操作，多个client并发操作，按照先到先执行的原则，先到的先执行，其余的阻塞。当然，另外的解决方案是把redis.set操作放在队列中使其串行化，必须的一个一个执行。

#### 缓存预热

缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。

这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题，用户直接查询事先被预热的缓存数据。

**解决思路**

- 直接写个缓存刷新页面，上线时手工操作下。
- 数据量不大，可以在项目启动的时候自动进行加载。

#### 缓存击穿

缓存击穿实际上是缓存雪崩的一个特例，大家使用过微博的应该都知道，微博有一个热门话题的功能，用户对于热门话题的搜索量往往在一些时刻会大大的高于其他话题，这种我们成为系统的“热点“，由于系统中对这些热点的数据缓存也存在失效时间，在热点的缓存到达失效时间时，此时可能依然会有大量的请求到达系统，没有了缓存层的保护，这些请求同样的会到达db从而可能引起故障。击穿与雪崩的区别即在于击穿是对于特定的热点数据来说，而雪崩是全部数据。**击穿与雪崩的区别即在于击穿是对于某一特定的热点数据来说，而雪崩是全部数据**。

**解决方案**

- 后台刷新:后台定义一个job(定时任务)专门主动更新缓存数据.比如,一个缓存中的数据过期时间是30分钟,那么job每隔29分钟定时刷新数据(将从数据库中查到的数据更新到缓存中)。

  这种方案比较容易理解，但会增加系统复杂度。比较适合那些 key 相对固定,cache 粒度较大的业务，key 比较分散的则不太适合，实现起来也比较复杂。

- 检查更新：将缓存key的过期时间(绝对时间)一起保存到缓存中(可以拼接,可以添加新字段,可以采用单独的key保存.不管用什么方式,只要两者建立好关联关系就行).在每次执行get操作后,都将get出来的缓存过期时间与当前系统时间做一个对比,如果缓存过期时间-当前系统时间<=1分钟(自定义的一个值),则主动更新缓存.这样就能保证缓存中的数据始终是最新的(和方案一一样,让数据不过期.)

  这种方案在特殊情况下也会有问题。假设缓存过期时间是12:00，而 11:59 到 12:00这 1 分钟时间里恰好没有 get 请求过来，又恰好请求都在 11:30 分的时候高并发过来，那就悲剧了。这种情况比较极端，但并不是没有可能。因为“高并发”也可能是阶段性在某个时间点爆发。

- 分级缓存：采用 L1 (一级缓存)和 L2(二级缓存) 缓存方式，L1 缓存失效时间短，L2 缓存失效时间长。 请求优先从 L1 缓存获取数据，如果 L1缓存未命中则加锁，只有 1 个线程获取到锁,这个线程再从数据库中读取数据并将数据再更新到到 L1 缓存和 L2 缓存中，而其他线程依旧从 L2 缓存获取数据并返回。

  这种方式，主要是通过避免缓存同时失效并结合锁机制实现。所以，当数据更新时，只能淘汰 L1 缓存，不能同时将 L1 和 L2 中的缓存同时淘汰。L2 缓存中可能会存在脏数据，需要业务能够容忍这种短时间的不一致。而且，这种方案可能会造成额外的缓存空间浪费。

- 加锁

  1. 方法一

     ```java
         // 方法1:
         public synchronized List<String> getData01() {
             List<String> result = new ArrayList<String>();
             // 从缓存读取数据
             result = getDataFromCache();
             if (result.isEmpty()) {
                 // 从数据库查询数据
                 result = getDataFromDB();
                 // 将查询到的数据写入缓存
                 setDataToCache(result);
             }
             return result;
         }
     ```

     这种方式确实能够防止缓存失效时高并发到数据库,但是缓存没有失效的时候,在从缓存中拿数据时需要排队取锁,这必然会大大的降低了系统的吞吐量.

  2. 方法二

     ```java
     // 方法2:
         static Object lock = new Object();
      
         public List<String> getData02() {
             List<String> result = new ArrayList<String>();
             // 从缓存读取数据
             result = getDataFromCache();
             if (result.isEmpty()) {
                 synchronized (lock) {
                     // 从数据库查询数据
                     result = getDataFromDB();
                     // 将查询到的数据写入缓存
                     setDataToCache(result);
                 }
             }
             return result;
         }
     ```

     这个方法在缓存命中的时候,系统的吞吐量不会受影响,但是当缓存失效时,请求还是会打到数据库,只不过不是高并发而是阻塞而已.但是,这样会造成用户体验不佳,并且还给数据库带来额外压力.

  3. 方法3

     ```java
     //方法3
         public List<String> getData03() {
             List<String> result = new ArrayList<String>();
             // 从缓存读取数据
             result = getDataFromCache();
             if (result.isEmpty()) {
                 synchronized (lock) {
                 //双重判断,第二个以及之后的请求不必去找数据库,直接命中缓存
                     // 查询缓存
                     result = getDataFromCache();
                     if (result.isEmpty()) {
                         // 从数据库查询数据
                         result = getDataFromDB();
                         // 将查询到的数据写入缓存
                         setDataToCache(result);
                     }
                 }
             }
             return result;
         }
     ```

     双重判断虽然能够阻止高并发请求打到数据库,但是第二个以及之后的请求在命中缓存时,还是排队进行的.比如,当30个请求一起并发过来,在双重判断时,第一个请求去数据库查询并更新缓存数据,剩下的29个请求则是依次排队取缓存中取数据.请求排在后面的用户的体验会不爽.

  4. 方法四

     ```java
     static Lock reenLock = new ReentrantLock();
      
         public List<String> getData04() throws InterruptedException {
             List<String> result = new ArrayList<String>();
             // 从缓存读取数据
             result = getDataFromCache();
             if (result.isEmpty()) {
                 if (reenLock.tryLock()) {
                     try {
                         System.out.println("我拿到锁了,从DB获取数据库后写入缓存");
                         // 从数据库查询数据
                         result = getDataFromDB();
                         // 将查询到的数据写入缓存
                         setDataToCache(result);
                     } finally {
                         reenLock.unlock();// 释放锁
                     }
      
                 } else {
                     result = getDataFromCache();// 先查一下缓存
                     if (result.isEmpty()) {
                         System.out.println("我没拿到锁,缓存也没数据,先小憩一下");
                         Thread.sleep(100);// 小憩一会儿
                         return getData04();// 重试
                     }
                 }
             }
             return result;
         }
     ```

     最后使用互斥锁的方式来实现，可以有效避免前面几种问题.

当然,在实际分布式场景中，我们还可以使用 redis、tair、zookeeper 等提供的分布式锁来实现.但是,如果我们的并发量如果只有几千的话,何必杀鸡焉用牛刀呢?

针对业务系统，永远都是具体情况具体分析，没有最好，只有最合适。 最后，对于缓存系统常见的缓存满了和数据丢失问题，需要根据具体业务分析，通常我们采用LRU策略处理溢出，Redis的RDB和AOF持久化策略来保证一定情况下的数据安全。

### 缓存淘汰原理

我们常用缓存提升数据查询速度，由于缓存容量有限，当缓存容量到达上限，就需要删除部分数据挪出空间，这样新数据才可以添加进来。缓存数据不能随机删除，一般情况下我们需要根据某种算法删除缓存数据。常用淘汰算法有 LRU,LFU,FIFO

- FIFO类：First In First Out，先进先出。判断被存储的时间，离目前最远的数据优先被淘汰。
- LRU类：Least Recently Used，最近最少使用。判断最近被使用的时间，目前最远的数据优先被淘汰。
- LFU类：Least Frequently Used，最不经常使用。在一段时间内，数据被使用次数最少的，优先被淘汰。

- OPT：最佳替换算法（optional replacement）:此算法是用来评价其他算法的。永远不可实现。算法核心是算法所淘汰的数据是以后永远不会访问的，或者是在最长时间内不再访问的那条数据。但是人们无法预知未来哪个页面才会被访问。OPT算法即为命中率最高的算法。

#### FIFO

##### FIFO

**原理**： 按照“先进先出（First In，First Out）”的原理淘汰数据

**实现**：   

1. 新访问的数据插入FIFO队列尾部，数据在FIFO队列中顺序移动；
2. 淘汰FIFO队列头部的数据；

![UTOOLS1574600233461.png](https://i.loli.net/2019/11/24/TrikDbtnBoaXGKR.png)

**特点：**

- 命中率   命中率很低，因为命中率太低，实际应用中基本上不会采用。
- 复杂度   简单
- 代价    实现代价很小

#####  Second Chance

**原理**： FIFO改进版，如果被淘汰的数据之前被访问过，则给其第二次机会（Second Chance）

**实现**：每个数据会增加一个访问标志位，用于标识此数据放入缓存队列后是否被再次访问过。

A是FIFO队列中最旧的数据，且其放入队列后没有被再次访问，则A被立刻淘汰；否则如果放入队列后被访问过，则将A移到FIFO队列头，并且将访问标志位清除。

如果所有的数据都被访问过，则经过一次循环后就会按照FIFO的原则淘汰数据。

**特点：**

- 命中率   命中率比FIFO高。
- 复杂度  与FIFO相比，需要记录数据的访问标志位，且需要将数据移动
- 代价    实现代价比FIFO高

##### Clock

**原理**： Clock是Second Chance的改进版，通过一个环形队列，避免将数据在FIFO队列中移动

**实现**:当前指针指向C，如果C被访问过，则清除C的访问标志，并将指针指向D；如果C没有被访问过，则将新数据插入到C的位置,将指针指向D。

**特点：**

- 命中率   命中率比FIFO高，和Second Chance一样
- 复杂度  与FIFO相比，需要记录数据的访问标志位，且需要将数据指针移动
- 代价    实现代价比FIFO高，比Second Chance低

##### 对比

| 命中率 | Clock = Second Chance > FIFO |
| ------ | ---------------------------- |
| 复杂度 | Second Chance > Clock > FIFO |
| 代价   | Second Chance > Clock > FIFO |

FIFO算法在某些情况下会出现当所分配的物理内存增大，但是命中率反而更低的异常现象。这是因为无法判断将要访问哪个页面造成的，又称为Belady异常。仅有FIFO算法会产生这种异常。

#### LRU

**LRU** 是 Least Recently Used 的缩写，这种算法认为最近使用的数据是热门数据，下一次很大概率将会再次被使用。而最近很少被使用的数据，很大概率下一次不再用到。当缓存容量的满时候，优先淘汰最近很少使用的数据。

假设现在缓存内部数据如图所示：

<img src="https://i.loli.net/2019/11/24/nl6pqbuWMJk7K5x.png" alt="UTOOLS1574599835955.png" style="zoom:50%;" />

> 这里我们将列表第一个节点称为头结点，最后一个节点为尾结点。

当调用缓存获取 key=1 的数据，LRU 算法需要将 1 这个节点移动到头结点，其余节点不变，如图所示。

<img src="https://i.loli.net/2019/11/24/FCeu9Byp51EkiZs.png" alt="UTOOLS1574599857705.png" style="zoom:50%;" />

然后我们插入一个 key=8 节点，此时缓存容量到达上限，所以加入之前需要先删除数据。由于每次查询都会将数据移动到头结点，未被查询的数据就将会下沉到尾部节点，尾部的数据就可以认为是最少被访问的数据，所以删除尾结点的数据。

<img src="https://i.loli.net/2019/11/24/WJmzGMTEHXS7udI.png" alt="UTOOLS1574599906032.png" style="zoom:50%;" />

然后我们直接将数据添加到头结点。

<img src="https://i.loli.net/2019/11/24/kqVlEAZLaW18uIF.png" alt="UTOOLS1574599948776.png" style="zoom:50%;" />

这里总结一下 LRU 算法具体步骤：

- 新数据直接插入到列表头部
- 缓存数据被命中，将数据移动到列表头部
- 缓存已满的时候，移除列表尾部数据。

**算法实现**

上面例子中可以看到，LRU 算法需要添加头节点，删除尾结点。而链表添加节点/删除节点时间复杂度 O(1)，非常适合当做存储缓存数据容器。但是不能使用普通的单向链表，单向链表有几点劣势:

1. 每次获取任意节点数据，都需要从头结点遍历下去，这就导致获取节点复杂度为 O(N)。
2. 移动中间节点到头结点，我们需要知道中间节点前一个节点的信息，单向链表就不得不再次遍历获取信息。

针对以上问题，可以结合其他数据结构解决。

使用散列表存储节点，获取节点的复杂度将会降低为 O(1)。节点移动问题可以在节点中再增加前驱指针，记录上一个节点信息，这样链表就从单向链表变成了双向链表。

综上使用双向链表加散列表结合体，数据结构如图所示:

<img src="https://i.loli.net/2019/11/24/jfDvFh6Zxq3kta9.png" alt="UTOOLS1574600010328.png" style="zoom: 67%;" />

> 在双向链表中特意增加两个『哨兵』节点，不用来存储任何数据。使用哨兵节点，增加/删除节点的时候就可以不用考虑边界节点不存在情况，简化编程难度，降低代码复杂度。

LRU 算法实现代码如下，为了简化 key ，val 都认为 int 类型。

```java
public class LRUCache {

    Entry head, tail;
    int capacity;
    int size;
    Map<Integer, Entry> cache;


    public LRUCache(int capacity) {
        this.capacity = capacity;
        // 初始化链表
        initLinkedList();
        size = 0;
        cache = new HashMap<>(capacity + 2);
    }

    /**
     * 如果节点不存在，返回 -1.如果存在，将节点移动到头结点，并返回节点的数据。
     *
     * @param key
     * @return
     */
    public int get(int key) {
        Entry node = cache.get(key);
        if (node == null) {
            return -1;
        }
        // 存在移动节点
        moveToHead(node);
        return node.value;
    }

    /**
     * 将节点加入到头结点，如果容量已满，将会删除尾结点
     *
     * @param key
     * @param value
     */
    public void put(int key, int value) {
        Entry node = cache.get(key);
        if (node != null) {
            node.value = value;
            moveToHead(node);
            return;
        }
        // 不存在。先加进去，再移除尾结点
        // 此时容量已满 删除尾结点
        if (size == capacity) {
            Entry lastNode = tail.pre;
            deleteNode(lastNode);
            cache.remove(lastNode.key);
            size--;
        }
        // 加入头结点

        Entry newNode = new Entry();
        newNode.key = key;
        newNode.value = value;
        addNode(newNode);
        cache.put(key, newNode);
        size++;

    }

    private void moveToHead(Entry node) {
        // 首先删除原来节点的关系
        deleteNode(node);
        addNode(node);
    }

    private void addNode(Entry node) {
        head.next.pre = node;
        node.next = head.next;

        node.pre = head;
        head.next = node;
    }

    private void deleteNode(Entry node) {
        node.pre.next = node.next;
        node.next.pre = node.pre;
    }


    public static class Entry {
        public Entry pre;
        public Entry next;
        public int key;
        public int value;

        public Entry(int key, int value) {
            this.key = key;
            this.value = value;
        }

        public Entry() {
        }
    }

    private void initLinkedList() {
        head = new Entry();
        tail = new Entry();

        head.next = tail;
        tail.pre = head;

    }

    public static void main(String[] args) {

        LRUCache cache = new LRUCache(2);

        cache.put(1, 1);
        cache.put(2, 2);
        System.out.println(cache.get(1));
        cache.put(3, 3);
        System.out.println(cache.get(2));

    }
}
```

**算法分析**

缓存命中率是缓存系统的非常重要指标，如果缓存系统的缓存命中率过低，将会导致查询回流到数据库，导致数据库的压力升高。

结合以上分析 LRU 算法优缺点。

LRU 算法优势在于算法实现难度不大，对于对于热点数据， LRU 效率会很好。

LRU 算法劣势在于对于偶发的批量操作，比如说批量查询历史数据，就有可能使缓存中热门数据被这些历史数据替换，造成缓存污染，导致缓存命中率下降，减慢了正常数据查询。

##### 改进方案

> 以下方案来源与 MySQL InnoDB LRU 改进算法

将链表拆分成两部分，分为热数据区，与冷数据区，如图所示。

![UTOOLS1574600108798.png](https://i.loli.net/2019/11/24/1pu9fB6i2AbUsjg.png)

改进之后算法流程将会变成下面一样:

1. 访问数据如果位于热数据区，与之前 LRU 算法一样，移动到热数据区的头结点。
2. 插入数据时，若缓存已满，淘汰尾结点的数据。然后将数据**插入冷数据区**的头结点。
3. 处于冷数据区的数据每次被访问需要做如下判断：
   - 若该数据已在缓存中超过指定时间，比如说 1 s，则移动到热数据区的头结点。
   - 若该数据存在在时间小于指定的时间，则位置保持不变。

对于偶发的批量查询，数据仅仅只会落入冷数据区，然后很快就会被淘汰出去。热门数据区的数据将不会受到影响，这样就解决了 LRU 算法缓存命中率下降的问题。

其他改进方法还有 LRU-K，2Q,LIRS 算法，感兴趣同学可以自行查阅。

**LCR-K**

为了解决“缓存污染”问题。K代表最近使用的次数。其核心思想是将“最近使用过1次”的判断标准扩展为“最近使用过K次”。

相比LRU，LRU-K需要多维护一个队列，用于记录所有缓存数据被访问的历史。只有当数据的访问次数达到K次的时候，才将数据放入缓存。当需要淘汰数据时，LRU-K会根据LRU算法淘汰缓存链表尾部的数据。

缓存污染：是指系统将不常用的数据从内存移到缓存，造成常用数据的挤出，降低了缓存效率的现象。

![](https://i.loli.net/2019/11/24/kV1sEBUt6xdMYTX.png)

实现过程：

1. 数据第一次被访问，加入到访问历史列表；
2. 如果数据在访问历史列表里后没有达到K次访问，则按照一定规则（FIFO，LRU）淘汰；
3. 当访问历史队列中的数据访问次数达到K次后，将数据索引从历史队列删除，将数据移到缓存队列的头部，缓存此数据；
4. 缓存数据队列中被再次访问后，重新排序；
5. 需要淘汰数据时，淘汰缓存队列中排在末尾的数据

即：访问历史可按照FIFO算法或LRU算法实现。缓存链表按LRU算法实现。

LRU-K具有LRU的优点，同时能够避免LRU的缺点，实际应用中LRU-2是综合各种因素后最优的选择，LRU-3或者更大的K值命中率会高，但适应性差，需要大量的数据访问才能将历史访问记录清除掉。

 **2Q（Two queues）**

类似于LRU-2。访问历史队列使用FIFO算法规则。缓存链表使用LRU算法规则。

### 扩展阅读

1. [Redis查漏补缺：Redis错过的技术要点大扫盲](https://stor.51cto.com/art/201812/588220.htm)
2. [缓存淘汰算法](https://www.jianshu.com/p/b21882c4f8e0)

#### 参考资料

**CND资料**

1. 淘宝CDN系统架构:http://blog.sina.com.cn/s/blog_4adf62ab0100tjld.html
2. 天猫浏览型应用的CDN静态化架构演变【经典】http://kb.cnblogs.com/page/199235/

3. ChinaCache CDN简介http://wenku.baidu.com/link?url=oAT72EEemiRnH2Iy2Bg4phHXsRmSlN_WHd4jH7kiDb4TqYMIyCR3v7oUhKMj9GqN7W1qwu1K4tQNyD6NKtuQ7o7aT3JIujcd_QjRf34BtKO


**反向代理资料**

1. squid反向代理：http://my.oschina.net/u/267384/blog/173149

**分布式缓存资料**

1. Memcache知识点梳理：http://369369.blog.51cto.com/319630/833234/
2. memcache学习总结-wish：http://wenku.baidu.com/link?url=Qx4JYNgBJN0pqREImt1mZr625sj03AJoCWsIwDZlFQfi1iyejCb0feqG0gov3FLcrtEioJ3fU-4zj0H6VKPXWONYVZaAyX-HPWXDbRxyqF7

3. memcache 分布式，算法实现：http://1006836709.iteye.com/blog/1997381

4. 分析Redis架构设计：http://blog.csdn.net/a600423444/article/details/8944601

5. Redis 集群方案：http://www.cnblogs.com/lulu/archive/2013/06/10/3130878.html

6. Redis常用数据类型：http://blog.sina.com.cn/s/blog_7f37ddde0101021q.html

