---
title: ZooKeeper 入门
date: 2020-04-17 18:18:59
tags:
 - 分布式
 - ZooKeeper
categories:
 - 分布式
 - ZooKeeper
---

zookeeper 是一个分布式的，开放源码的分布式应用程序协调服务，是 google chubby 的开源实现，是 hadoop 和 hbase 的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。

<!--more-->

 **zookeeper 都有哪些功能？**

- 集群管理：监控节点存活状态、运行请求等。
- 主节点选举：主节点挂掉了之后可以从备用的节点开始新一轮选主，主节点选举说的就是这个选举的过程，使用 zookeeper 可以协助完成这个过程。
- 分布式锁：zookeeper 提供两种锁：独占锁、共享锁。独占锁即一次只能有一个线程使用资源，共享锁是读锁共享，读写互斥，即可以有多线线程同时读同一个资源，如果要使用写锁也只能有一个线程使用。zookeeper可以对分布式锁进行控制。
- 命名服务：在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。

**zookeeper 有几种部署模式？**

zookeeper 有三种部署模式：

- 单机部署：一台集群上运行；
- 集群部署：多台集群运行；
- 伪集群部署：一台集群启动多个 zookeeper 实例运行。

#### ZooKeeper语义保证

保证如下分布式一致性特性

1. 顺序一致性

   从同一个客户端发起的事务请求，最终将会严格地按照其发生顺序被应用到ZooKeeper中。

2. 原子一致性

   所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，即要么整个集群所有机器都成功应用了某一个事务，要么都没有应用，一定不会出现集群中部分机器应用了该事务，而另外一部分没有应用的情况。

3. 单一视图

   无论客户端连接的是哪个ZK服务器，其看到的服务端数据模型都是一致的。

4. 可靠性

   一旦服务端成功地应用了一个事务并完成对客户端的响应，那么该事务所引起的服务端状态变更将会被一直保留下来，除非另外一个事务又对其进行了变更。

5. 实时性

   通常人们看到实时性的第一反应是，一旦一个事务被成功应用，那么客户端能够立即从服务端上读取到这个事务变更后的最新数据状态。但这个需要注意的是，ZooKeeper仅仅保证在一定时间段内，客户端最终一定能够从服务端上读取到最新的数据状态。


### 核心概念

#### 分布式系统面临的问题

我们将分布式系统定义为：分布式系统是同时跨越多个物理主机，独立运行的多个软件所组成系统。类比一下，分布式系统就是一群人一起干活。人多力量大，每个服务器的算力是有限的，但是通过分布式系统，由n个服务器组成起来的集群，算力是可以无限扩张的。

优点显而易见，人多干活快，并且互为备份。但是缺点也很明显。我们可以想象一下，以一个小研发团队开发软件为例，假设我们有一个5人的项目组，要开始一个系统的开发，项目组将面临如下问题：

![lJXgVH.png](https://s2.ax1x.com/2020/01/02/lJXgVH.png)

你一定在想，以上这些问题很简单啊，在我的日常工作中天天都在发生，并没感觉有什么复杂。是的，这是因为我们人类的大脑是个超级计算机，能够灵活应对这些问题，而且现实中信息的交换不依赖网络，不会因网络延迟或者中断，出现信息不对等。而且现实中对以上问题的处理其实并不严谨，从而也引发了很多问题。想一想，项目中是不是出现过沟通不畅造成任务分配有歧义？是否由于人员离职造成任务进行不下去，甚至要联系离职人员协助？是不是出现过任务分配不合理？类似这样的各种问题，肯定会发生于你的项目组中。在现实世界，我们可以人为去协调，即使出错了，人工去补错，加加班搞定就好。但在计算机的世界，这样做是行不通的，一切都要保证严谨，以上问题要做到尽可能不要发生。因此，分布式系统必须采用合理的方式解决掉以上的问题。

实际上要想解决这些问题并没有那么复杂，我们仅需要做一件事就可以万事无忧---让信息在项目组成员中同步。如果能做到信息同步，那么每个人在干什么，大家都是清楚的，干到什么程度也是清晰的，无论谁离职也不会产生问题。分配的工作，能够及时清晰的同步给每个组员，确保每个组员收到的任务分配没有冲突。

分布式系统的协调工作就是通过某种方式，让每个节点的信息能够同步和共享。这依赖于服务进程之间的通信。通信方式有两种：

1. 通过网络进行信息共享

   这就像现实世界，开发leader在会上把任务传达下去，组员通过听leader命令或者看leader的邮件知道自己要干什么。当任务分配有变化时，leader会单独告诉组员，或者再次召开会议。信息通过人与人之间的直接沟通，完成传递。

2. 通过共享存储

   这就好比开发leader按照约定的时间和路径，把任务分配表放到了svn上，组员每天去svn上拉取最新的任务分配表，然后干活。其中svn就是共享存储。更好一点的做法是，当svn文件版本更新时，触发邮件通知，每个组员再去拉取最新的任务分配表。这样做更好，因为每次更新，组员都能第一时间得到消息，从而让自己手中的任务分配表永远是最新的。此种方式依赖于中央存储。整个过程如下图所示：

<img src="https://s2.ax1x.com/2020/01/02/lJXbZQ.png" alt="lJXbZQ.png" style="zoom:50%;" />

#### ZooKeeper如何解决分布式系统面临的问题

ZooKeeper对分布式系统的协调，使用的是第二种方式，共享存储。其实共享存储，分布式应用也需要和存储进行网络通信。网络通信是分布式系统并发设计的基础。

实际上，通过ZooKeeper实现分布式协同的原理，和项目组通过SVN同步工作任务的例子是一样的。ZooKeeper就像是svn，存储了任务的分配、完成情况等共享信息。每个分布式应用的节点就是组员，订阅这些共享信息。当主节点（组leader），对某个从节点的分工信息作出改变时，相关订阅的从节点得到zookeeper的通知，取得自己最新的任务分配。完成工作后，把完成情况存储到zookeeper。主节点订阅了该任务的完成情况信息，所以将得到zookeeper的完工的通知。参考下图，回味一下，是不是和前面项目组通过svn分配工作的例子一模一样？仅仅是把svn和邮件系统合二为一，以ZooKeeper代替。

<img src="https://s2.ax1x.com/2020/01/02/lJjidJ.png" alt="lJjidJ.png" style="zoom:50%;" />

注：Slave节点要想获取ZooKeeper的更新通知，需事先在关心的数据节点上设置观察点。

大多数分布式系统中出现的问题，都源于信息的共享出了问题。如果各个节点间信息不能及时共享和同步，那么就会在协作过程中产生各种问题。ZooKeeper解决协同问题的关键，在于保证分布式系统信息的一致性。

通过以上的讲解，我们应该已经理解分布式系统以及其面临的问题。了解了ZooKeeper通过什么样的机制去解决这些问题。从宏观上对ZooKeeper已经有了认知，接下来我们先切入到zookeeper自身，讲解zookeeper的概念，这些概念很重要，所有zookeeper的应用都会围绕这些概念来实现。

#### zookeeper概念介绍

ZooKeeper并不直接暴露分布式服务所需要的原语及原语的调用方法。什么是原语？举个例子，比如说分布式锁机制是一个原语，它会暴露出创建、获取、释放三个调用方法。ZooKeeper以类似文件系统的方式存储数据，暴漏出调用这些数据的API。让应用通过ZooKeeper的机制和API，自己来实现分布式相关原语。

我们若想让应用能够通过ZooKeeper实现分布式协同，那么第一件事就是了解ZooKeeper的特性及相关概念，另外熟悉它给我们提供了哪些API。

##### znode

上面讲过Zookeeper会保存任务的分配、完成情况，等共享信息，那么ZooKeeper是如何保存的呢？在ZooKeeper中，这些信息被保存在一个个数据节点上，这些节点被称为znode。它采用了类似文件系统的层级树状结构进行管理。见下图示例：

![lJjFo9.png](https://s2.ax1x.com/2020/01/02/lJjFo9.png)

根节点/包含4个子节点，其中三个拥有下一级节点。有的叶子节点存储了信息。

节点上没有存储数据，也有着重要的含义。比如在主从模式中，当/master节点没有数据时，代表分布式应用的主节点还没有选举出来。

znode节点存储的数据为字节数组。存储数据的格式zookeeper不做限制，也不提供解析，需要应用自己实现。

实际上图就是主从模式存储数据的示例，这里先简单讲解：

1. /master，存储了当前主节点的信息
2. /workers，下面的每个子znode代表一个从节点，子znode上存储的数据，如“foo.com:2181”，代表从节点的信息。
3. /tasks，下面的每个子znode代表一个任务，子znode上存储的信息如“run cmd”，代表该内务内容
4. /assign，下面每个子znode代表一个从节点的任务集合。如/assign/worker-1，代表worker-1这个从节点的任务集合。/assign/worker-1下的每个子znode代表分配给worker-1的一个任务。

**持久节点（persistent）和临时节点（ephemeral）**

持久节点只能通过delete删除。临时节点在创建该节点的客户端崩溃或关闭时，自动被删除。

前面例子中的/master应该使用临时节点，这样当主节点失效或者退出时，该znode被删除，其他节点知道主节点崩溃了，开始进行选举的逻辑。另外/works/worker-1也应该是临时节点，在此从节点失效的时候，该临时节点自动删除。

在目前的版本，由于临时znode会因为创建者会话过期被删除，所以不允许临时节点拥有子节点。

**有序节点**

znode可以被设置为有序（sequential）节点。有序znode节点被分配唯一一个单调递增的证书。如果创建了个一有序节点为/workers/worker-，zookeeper会自动分配一个序号1，追加在名字后面，znode名称为/workers/worker-1。通过这种方式，可以创建唯一名称znode，并且可以直观的看到创建的顺序。

**znode支持的操作及暴露的API：**

- create /path data：创建一个名为/path的znode，数据为data。

- delete /path：删除名为/path的znode。

- exists /path：检查是否存在名为/path的znode

- setData /path data：设置名为/path的znode的数据为data

- getData /path：返回名为/path的znode的数据

- getChildren /path：返回所有/path节点的所有子节点列表


##### 观察与通知

分布式应用需要及时知道zookeeper中znode的变化，从而了解到分布式应用整体的状况，如果采用轮询方式，代价太大，绝大多数查询都是无效的。因此，zookeeper采用了通知的机制。客户端向zookeeper请求，在特定的znode设置观察点（watch）。当该znode发生变化时，会触发zookeeper的通知，客户端收到通知后进行业务处理。观察点触发后立即失效。所以一旦观察点触发，需要再次设置新的观察点。

我们使用Zookeeper不能期望能够监控到节点每次的变化。思考如下场景：

1. 客户端C1设置观察点在/tasks

2. 观察点触发，C1处理自己的逻辑

3. C1设置新的观察点前，C2更新了/tasks

4. C1处理完逻辑，再次设置了观察点。

此时C1不会得到第三步的通知，因此错过了C2更新/tasks这次操作。要想不错过这次更新，C1需要在设置监视点前读取/tasks的数据，进行对比，发现更新。

再如下面的场景：

1. 客户端C1设置观察点在/tasks

2. /tasks上发生了连续两次更新

3. C1在得到第一次更新的通知后就读取了/tasks的数据

4. 此时第二次更新也已经发生，C1用第一次的通知，读取到两次更新后的数据

此时C1虽然错过了第二次通知，但是C1最终还是读取到了最新的数据。

因此Zookeeper只能保证最终的一致性，而无法保证强一致性。

zookeeper可以定义不同的观察类型。例如观察znode数据变化，观察znode子节点变化，观察znode创建或者删除。

##### 版本

每个znode都有版本号，随着每次数据变化自增。setData和delete，以版本号作为参数，当传入的版本号和服务器上不一致时，调用失败。当多个zookeeper客户端同时对一个znode操作时，版本将会起到作用，假设c1，c2同时往一个znode写数据，c1先写完后版本从1升为2，但是c2写的时候携带版本号1，c2会写入失败。

##### 法定人数

zookeeper服务器运行于两种模式：独立模式和仲裁模式（集群）。仲裁模式下，会复制所有服务器的数据树。但如果让客户端等待所有复制完成，延迟太高。这里引入法定人数概念，指为了使zookeeper集群正常工作，必须有效运行的服务器数量。同时也是服务器通知客户端保存成功前，必须保存数据的服务器最小数。例如我们有一个5台服务器的zookeeper集群，法定人数为3，只要任何3个服务器保存了数据，客户端就会收到确认。只要有3台服务器存活，整个zookeeper集群就是可用的。

下图展示了客户端提交请求到收到回复的过程：

![lJjAiR.png](https://s2.ax1x.com/2020/01/02/lJjAiR.png)

法定人数需要大于服务器数量的一半。也称为多数原则。举个例子说明，假如集群有5台服务器，法定人数为2，那么有2台服务器参与复制即可，若这2台server刚刚复制完/z这个znode，就挂掉了。此时剩下了3台server，大于法定人数2，所以zookeeper认为集群正常，但这三台服务器是无法发现/z这个znode的。如果法定人数大于服务器数量一半，那么法定人数复制完成，就可以确保集群存活时，至少有一台服务器有最新的znode，否则集群认为自己已经崩溃。

下面两个例子阐明了，为何要遵循多数原则。

下图展示了5台server，法定人数为3，在确保zookeeper集群存活的前提下，最坏的情况挂了2台server（剩余及器数量3>=法定人数3），zookeeper是如何能确保数据完备，集群继续工作的。

![lJjEJ1.png](https://s2.ax1x.com/2020/01/02/lJjEJ1.png)

接下来两张图展示了5台server，未遵循多数原则，法定人数设为2。同样挂了两台server时，为什么zookeeper集群会出问题。

首先，客户端发起请求，2个server复制数据后即返回客户端接收成功

![lJjmQK.png](https://s2.ax1x.com/2020/01/02/lJjmQK.png)

就在此刻，很不幸，在继续同步更新给其他节点前，刚刚两个复制了数据的节点挂了。此时会怎样呢？如下图：

![lJjnsO.png](https://s2.ax1x.com/2020/01/02/lJjnsO.png)

可以看到创建/z的操作在zookeeper集群中丢失了。

相信通过以上讲解，你已经能够理解为什么法定人数一定要多于一半服务器的数量。

此外，我们要尽量选用奇数个服务器，这样集群能容忍崩溃服务器占比更大，性价比更高。例如4台服务器的集群，法定人数最少为3，那么只能允许1台服务器崩溃，也就是仅允许25%的机器崩溃。而5台服务器的集群，法定人数最少也是3，但是此时允许2台服务器崩溃。换句话讲，40%的机器崩溃后还能工作。

仲裁模式下，负载均衡通过客户端随机选择连接串中的某个服务器来实现。

##### 会话

客户端对zookeeper集群发送任何请求前，需要和zookeeper集群建立会话。客户端提交给zookeeper的所有操作均关联在一个会话上。当一个会话因某种原因终止时，会话期间创建的临时节点将会消失。而当当前服务器的问题，无法继续通信时，会话将被透明的转移到另外一台zookeeper集群的服务器上。

会话提供了顺序保障。同一个会话中的请求以FIFO顺序执行。并发会话的FIFO顺序无法保证。

##### 会话状态和生命周期

会话状态有：

connecting、connected、closed、not_connected

创建会话时，需要设置会话超时这个重要的参数。如果经过时间t后服务接受不到这个会话的任何消息，服务就会声明会话过期。客户端侧，t/3时间未收到任何消息，客户端向服务器发送心跳消息，2t/3时间后，客户端开始寻找其他服务器。此时他有t/3的时间去寻找，找不到的话，会话失效。

重连服务器时，只有更新大于客户端的服务器才能被连接，以免连接到落后的服务器。zookeeper中通过更新建立的顺序，分配事务标识符。只有服务器的事物标识符大于客户端携带的标识符时，才可连接。

##### 事务操作

在ZooKeeper中，能改变ZooKeeper服务器状态的操作称为事务操作。一般包括数据节点创建与删除、数据内容更新和客户端会话创建与失效等操作。对应每一个事务请求，ZooKeeper都会为其分配一个全局唯一的事务ID，用ZXID表示，通常是一个64位的数字。每一个ZXID对应一次更新操作，从这些ZXID中可以间接地识别出ZooKeeper处理这些事务操作请求的全局顺序。

##### Watcher

Watcher（事件监听器），是ZooKeeper中一个很重要的特性。ZooKeeper允许用户在指定节点上注册一些Watcher，并且在一些特定事件触发的时候，ZooKeeper服务端会将事件通知到感兴趣的客户端上去。该机制是ZooKeeper实现分布式协调服务的重要特性。

##### ACL

ZooKeeper采用ACL（Access Control Lists）策略来进行权限控制。ZooKeeper定义了如下5种权限。

- CREATE: 创建子节点的权限。
- READ: 获取节点数据和子节点列表的权限。
- WRITE：更新节点数据的权限。
- DELETE: 删除子节点的权限。
- ADMIN: 设置节点ACL的权限。

注意：CREATE 和 DELETE 都是针对子节点的权限控制。

#### ZAB协议

ZooKeeper是Chubby的开源实现，而Chubby是Paxos的工程实现，所以很多人以为ZooKeeper也是Paxos算法的工程实现。事实上，ZooKeeper并没有完全采用Paxos算法，而是使用了一种称为**ZooKeeper Atomic Broadcast（ZAB，ZooKeeper原子广播协议）**的协议作为其**数据一致性**的核心算法。

ZAB协议并不像Paxos算法和Raft协议一样，是通用的分布式一致性算法，它是一种特别为ZooKeeper设计的崩溃可恢复的原子广播算法。

接下来对ZAB协议做一个浅显的介绍，目的是让大家对ZAB协议有个直观的了解。读者不用太纠结于细节。至于更深入的细节，以后再专门分享。

基于ZAB协议，ZooKeeper实现了一种主备模式（Leader、Follower）的系统架构来保持集群中各副本之间数据的一致性。

具体的，ZooKeeper使用了一个**单一的主进程**（Leader）来接收并处理客户端的所有**事务请求**，并采用ZAB的原子广播协议，将服务器数据的**状态变更**以事务Proposal的形式广播到所有的副本进程上去（Follower）。ZAB协议的这个主备模型架构保证了同一时刻集群中只能有一个主进程来广播服务器的状态变更，因此能够很好地处理客户端大量的并发请求。另一方面，考虑到分布式环境中，顺序执行的一些状态变更其前后会存在一定的依赖关系，有些状态变更必须依赖于比它早生成的那些状态变更，例如变更C需要依赖变更A和变更B。这样的依赖关系也对ZAB协议提出了一个要求：ZAB协议必须能够保证一个**全局的变更序列**被**顺序应用**。也就是说，ZAB协议需要保证如果一个状态变更已经被处理了，那么所有依赖的状态变更都应该已经被提前处理掉了。最后，考虑到**主进程**在任何时候都有可能出现**崩溃**退出或**重启**现象，因此，ZAB协议还需要做到在当前主进程出现上述**异常**情况的时候，依然能够**正常工作**。

ZAB协议的**核心**是定义了对应那些会改变ZooKeeper服务器数据状态的**事务请求**的处理方式，即：

> 所有事务请求必须由一个**全局唯一**的服务器来协调处理，这样的服务器被称为**Leader服务器**，而剩下的其他服务器则成为**Follower服务器**。Leader服务器负责将一个客户端**事务请求**转换成一个**事务Proposal（提案）**并将该Proposal分发给集群中**所有**的Follower服务器。之后Leader服务器需要等待**所有**Follower服务器的**反馈**，一旦**超过半数**的Follower服务器进行了**正确的反馈**后，Leader就会再次向所有的Follower服务器**分发Commit消息**，要求对刚才的Proposal进行**提交**。

ZAB协议包括两种基本的模式，分别是**崩溃恢复**和**消息广播**。在整个ZooKeeper**集群启动**过程中，或是当**Leader服务器**出现**网络中断、崩溃退出与重启**等异常情况时，ZAB协议就会进入**恢复模式**并**选举产生新的Leader服务器**。当选举产生了新的Leader服务器，同时集群中有**过半**的机器与该Leader服务器完成了**状态同步**之后，ZAB协议就会**退出恢复模式**。其中，状态同步是指数据同步，用来保证集群中存在**过半的机器**能够和Leader服务器的**数据状态保持一致**。

崩溃恢复模式包括两个阶段：**Leader选举**和**数据同步**。

当集群中有过半的Follower服务器完成了和Leader服务器的状态同步，那么整个集群就可以进入**消息广播模式**了。

##### ZooKeeper典型应用场景

ZooKeeper是一个**高可用**的分布式**数据管理与协调框架**。基于对ZAB算法的实现，该框架能够很好地保证分布式环境中数据的**一致性**。也是基于这样的特性，使得ZooKeeper成为了解决分布式一致性问题的利器。

1. 数据发布与订阅（配置中心）

   数据发布与订阅，即所谓的**配置中心**，顾名思义就是发布者将数据发布到ZooKeeper节点上，供订阅者进行数据订阅，进而达到**动态获取数据**的目的，实现配置信息的**集中式管理**和**动态更新**。

   在我们平常的应用系统开发中，经常会碰到这样的需求：系统中需要使用一些通用的配置信息，例如**机器列表信息**、**数据库配置信息**等。这些全局配置信息通常具备以下3个特性。

   - 数据量通常比较**小。**
   - 数据内容在运行时**动态变化**。
   - 集群中各机器共享，**配置一致**。

   对于这样的全局配置信息就可以发布到ZooKeeper上，让客户端（集群的机器）去订阅该消息。

   发布/订阅系统一般有两种设计模式，分别是**推（Push）**和**拉（Pull）**模式。

   - 推：**服务端主动**将数据更新发送给所有订阅的客户端。
   - 拉：**客户端主动**发起请求来获取最新数据，通常客户端都采用**定时轮询**拉取的方式。

   ZooKeeper采用的是**推拉相结合**的方式。如下：

   客户端想服务端**注册**自己需要关注的节点，一旦该节点的数据发生**变更**，那么服务端就会向相应的客户端发送Watcher事件**通知**，客户端接收到这个消息通知后，需要**主动**到服务端**获取**最新的数据（**推拉结合**）。

2. 命名服务(Naming Service)

   命名服务也是分布式系统中比较常见的一类场景。在分布式系统中，通过使用命名服务，客户端应用能够根据指定**名字**来获取**资源或服务的地址，提供者等信息**。被命名的实体通常可以是**集群中的机器，提供的服务，远程对象等等**——这些我们都可以统称他们为**名字（Name）**。其中较为常见的就是一些分布式服务框架（如RPC、RMI）中的服务地址列表。通过在ZooKeepr里创建顺序节点，能够很容易创建一个**全局唯一的路径**，这个路径就可以作为一个**名字**。

   ZooKeeper的命名服务即生成**全局唯一的ID**。

3. 分布式协调/通知

   ZooKeeper中特有**Watcher注册**与**异步通知机制**，能够很好的实现分布式环境下不同机器，甚至不同系统之间的**通知与协调**，从而实现**对数据变更的实时处理**。使用方法通常是不同的客户端都对ZK上同一个ZNode进行注册，监听ZNode的变化（包括ZNode本身内容及子节点的），如果ZNode发生了变化，那么所有订阅的客户端都能够接收到相应的Watcher通知，并做出相应的处理。

   **ZK的分布式协调/通知，是一种通用的分布式系统机器间的通信方式**。

4. 心跳检测

   机器间的心跳检测机制是指在分布式环境中，不同机器（或进程）之间需要检测到彼此是否在正常运行，例如A机器需要知道B机器是否正常运行。在传统的开发中，我们通常是通过主机直接是否可以**相互PING通**来判断，更复杂一点的话，则会通过在机器之间建立长连接，通过**TCP连接**固有的心跳检测机制来实现上层机器的心跳检测，这些都是非常常见的心跳检测方法。

   下面来看看如何使用ZK来实现分布式机器（进程）间的心跳检测。

   基于ZK的**临时节点**的特性，可以让不同的进程都在ZK的一个**指定节点**下创建**临时子节点**，不同的进程直接可以根据这个临时子节点来判断对应的进程**是否存活**。通过这种方式，检测和被检测系统直接并不需要直接相关联，而是通过ZK上的某个节点进行关联，大大**减少了系统耦合**。

5. 工作进度汇报

   在一个常见的**任务分发系统**中，通常任务被分发到不同的机器上执行后，需要实时地将自己的任务执行进度**汇报**给分发系统。这个时候就可以通过ZK来实现。在ZK上选择一个节点，每个任务客户端都在这个节点下面创建**临时子节点**，这样便可以实现两个功能：

   - 通过判断临时节点是否存在来确定任务机器**是否存活**。
   - 各个任务机器会实时地将自己的**任务执行进度写到这个临时节点上去**，以便中心系统能够实时地获取到任务的**执行进度**。

6. Master选举

   **Master选举**可以说是ZooKeeper**最典型的应用场景**了。比如HDFS中Active NameNode的选举、YARN中Active ResourceManager的选举和HBase中Active HMaster的选举等。

   针对Master选举的需求，通常情况下，我们可以选择常见的**关系型数据库**中的**主键特性**来实现：希望成为Master的机器都向数据库中插入一条**相同主键ID**的记录，数据库会帮我们进行**主键冲突检查**，也就是说，**只有一台**机器能插入成功——那么，我们就认为向数据库中**成功插入**数据的客户端机器**成为Master**。

   依靠关系型数据库的主键特性确实能够很好地保证在集群中选举出唯一的一个Master。但是，如果当前选举出的Master挂了，那么该如何处理？谁来告诉我Master挂了呢？显然，关系型数据库无法通知我们这个事件。但是，ZooKeeper可以做到！

   利用ZooKeepr的强一致性，能够很好地保证在分布式高并发情况下节点的创建一定能够保证全局唯一性，即ZooKeeper将会保证客户端**无法创建一个已经存在的ZNode**。也就是说，如果同时有多个客户端请求创建**同一个**临时节点，那么最终一定**只有一个**客户端请求能够创建成功。利用这个特性，就能很容易地在分布式环境中进行Master选举了。

   成功创建该节点的客户端所在的机器就成为了Master。同时，其他没有成功创建该节点的客户端，都会在该节点上**注册**一个子节点变更的**Watcher**，用于监控当前Master机器是否存活，一旦发现当前的Master挂了，那么其他客户端将会**重新进行Master选举**。

   这样就实现了Master的**动态选举**。

7. 分布式锁

   分布式锁是控制**分布式系统**之间**同步访问共享资源**的一种方式。

### 安装和使用

1. 下载

   首先我们下载最新稳定版本的zookeeper

   http://apache.fayea.com/zookeeper/stable/

2. 解压:下载完成后，我们解开压缩包

3. 创建配置文件

   解压后的路径下找到conf文件夹，进入conf文件夹复制zoo_sample.cfg，命名为zoo.cfg

   ```properties
   # 每个tick的毫秒数。后面的initLimit和syncLimit都以tick为单位
   tickTime=2000
    
   #初始化同步阶段时长
   initLimit=10
    
   # 同步确认时长
   syncLimit=5
    
   # 快照存储路径，不要使用/tmp。这里使用只是为了示例
   dataDir=/tmp/zookeeper
    
   # 客户端端口好
   clientPort=2181
    
   #可连接的client的最大数
   #如果需要处理更多的clinet，请增加此数量
   #maxClientCnxns=60
   #
   # Be sure to read the maintenance section of the 
   # administrator guide before turning on autopurge.
   #
   # http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance
   #
   # The number of snapshots to retain in dataDir
   #autopurge.snapRetainCount=3
   # Purge task interval in hours
   # Set to "0" to disable auto purge feature
   #autopurge.purgeInterval=1
   ```

   对于学习来说，一般无需做修改。生产环境，需要修改dataDir。其他参数后续调优。

4. 单机启动ZooKeeper

   在ZooKeeper根目录下执行 bin/zkServer.sh start

   ```bash
   $ ./bin/zkServer.sh start
   ZooKeeper JMX enabled by default
   Using config: /home/hyp/dev/zookeeper/bin/../conf/zoo.cfg
   Starting zookeeper ... STARTED
   ```

5. 通过客户端连接ZooKeeper

   ```bash
   $ ./bin/zkCli.sh
   ```

6. 通过客户端执行基本命令

   ```shell
   #创建znode，名为/my_test，携带数据testData
   [zk: localhost:2181(CONNECTED) 0] create /my_test testData
   #查看znode信息
   [zk: localhost:2181(CONNECTED) 2] get /my_test
   testData
   #修改znode数据
   [zk: localhost:2181(CONNECTED) 3] set /my_test testDataV2
   #创建子znode
   [zk: localhost:2181(CONNECTED) 0] create /my_test/tester1 testData
   #列出子znode
   [zk: localhost:2181(CONNECTED) 5] ls  /my_test
   [tester1]
   #删除znode,有子节点的znode不能直接删除，否则会报错。
   [zk: localhost:2181(CONNECTED) 6] delete /my_test
   Node not empty: /my_test
   #我们删除/my_test/tester1。
   [zk: localhost:2181(CONNECTED) 8] delete /my_test/tester1
   #再次查看该znode。
   [zk: localhost:2181(CONNECTED) 9] ls  /my_test
   []
   #退出
   [zk: localhost:2181(CONNECTED) 13] quit
   ```

#### 集群配置和启动

我们配置和启动一个单主机三个server的集群。

在一台机器上部署了3个server，需要注意的是在集群为分布式模式下我们使用的每个配置文档模拟一台机器，也就是说单台机器及上运行多个Zookeeper实例。但是，必须保证每个配置文档的各个端口号不能冲突，除了clientPort不同之外，dataDir也不同。另外，还要在dataDir所对应的目录中创建myid文件来指定对应的Zookeeper服务器实例。

- clientPort端口：如果在1台机器上部署多个server，那么每台机器都要不同的 clientPort，比如 server1是2181,server2是2182，server3是2183
- dataDir和dataLogDir：dataDir和dataLogDir也需要区分下，将数据文件和日志文件分开存放，同时每个server的这两变量所对应的路径都是不同的
- server.X和myid： server.X 这个数字就是对应，data/myid中的数字。在3个server的myid文件中分别写入了0，1，2，那么每个server中的zoo.cfg都配 server.0 server.2,server.3就行了。因为在同一台机器上，后面连着的2个端口，3个server都不要一样，否则端口冲突

1. 新建3个配置文件zoox.cfg文件

   ```properties
   # zoo1.cfg
   tickTime=2000
   initLimit=10
   syncLimit=5
   dataDir=/tmp/zk1/data
   clientPort=2182
   dataLogDir=/tmp/zk1/logs
   
   server.1=localhost:2287:3387
   server.2=localhost:2288:3388
   server.3=localhost:2289:3389
   
   #zoo2.cfg
   tickTime=2000
   initLimit=10
   syncLimit=5
   dataDir=/tmp/zk2/data
   clientPort=2183
   dataLogDir=/tmp/zk2/logs
   
   server.1=localhost:2287:3387
   server.2=localhost:2288:3388
   server.3=localhost:2289:3389
   
   #zoo3.cfg
   tickTime=2000
   initLimit=10
   syncLimit=5
   dataDir=/tmp/zk3/data
   clientPort=2184
   dataLogDir=/tmp/zk3/logs
   
   server.1=localhost:2287:3387
   server.2=localhost:2288:3388
   server.3=localhost:2289:3389
   ```

   initLimit，zookeeper用来限制zookeeper服务器连接到leader的时长。

   syncLimit，一个服务器多久在leader那里过期。

   以上两种过期时间，单位都是tickTime，

   本例initLimit时长为5个tickTime＝5*2000ms＝10秒

   server.x列出了所有的zookeeper服务。集群启动它通过查看data下面的myid来知道自己是哪台服务器。

   2888用来连接其它server。3888用来leader选举。

2. 在配置的dataDir路径下创建myid文件,文件内容为对应自己的server.x的x值。比如zoo1这台sever，myid文件中内容为1

   ```
   $ mkdir /tmp/{zk1,zk2,zk3}/{data,logs} -p
   ```

   

3. 启动每个zookeeper

   ```
   $ ./bin/zkServer.sh start ./conf/zoo1.cfg 
   $ ./bin/zkServer.sh start ./conf/zoo2.cfg 
   $ ./bin/zkServer.sh start ./conf/zoo3.cfg 
   ```

4. 查看本机状态，是leader还是follower

   ```
   $ ./bin/zkServer.sh status ./conf/zoo1.cfg 
   ```

   至此我们已经把zookeeper集群启动起来了，并且做了验证，有一台是leader，另外两台是follower。

   我们还可以做个实验，看zookeeper集群是否工作正常。

   ```shell
   #连接其中一台server，创建一个znode。
   $ bin/zkCli.sh -server 127.0.0.1:2182
   [zk: 127.0.0.1:2182(CONNECTED) 1] create /zk_test my_data
   Created /zk_test
   #连接另外一台server，列出所有根节点下znode。
   $ bin/zkCli.sh -server 127.0.0.1:2183
   [zk: 127.0.0.1:2183(CONNECTED) 0] ls /
   [zk_test, zookeeper]
   ```

   可以看到集群已经能够正常工作，server间可以正常进行数据的复制。

   客户端连接集群方法如下：

   ```shell
   $ bin/zkCli.sh -server 127.0.0.1:2183,127.0.0.1:2182,127.0.0.1:2184
   ```

   客户端会自动随机选择集中一台连接。

至此，ZooKeeper单机环境和集群环境的搭建讲解完成

##### 使用docker搭建

```shell
docker pull zookeeper

docker run -d \
 --name=zk1 \
 --net=host \
 -e SERVER_ID=1 \
 -e ADDITIONAL_ZOOKEEPER_1=server.1=localhost:2888:3888 \
 -e ADDITIONAL_ZOOKEEPER_2=server.2=localhost:2889:3889 \
 -e ADDITIONAL_ZOOKEEPER_3=server.3=localhost:2890:3890 \
 -e ADDITIONAL_ZOOKEEPER_4=clientPort=2181 \
 zookeeper

docker run -d \
 --name=zk2 \
 --net=host \
 -e SERVER_ID=2 \
 -e ADDITIONAL_ZOOKEEPER_1=server.1=localhost:2888:3888 \
 -e ADDITIONAL_ZOOKEEPER_2=server.2=localhost:2889:3889 \
 -e ADDITIONAL_ZOOKEEPER_3=server.3=localhost:2890:3890 \
 -e ADDITIONAL_ZOOKEEPER_4=clientPort=2182 \
 zookeeper

docker run -d \
 --name=zk3 \
 --net=host \
 -e SERVER_ID=3 \
 -e ADDITIONAL_ZOOKEEPER_1=server.1=localhost:2888:3888 \
 -e ADDITIONAL_ZOOKEEPER_2=server.2=localhost:2889:3889 \
 -e ADDITIONAL_ZOOKEEPER_3=server.3=localhost:2890:3890 \
 -e ADDITIONAL_ZOOKEEPER_4=clientPort=2183 \
 zookeeper
```

参考：[使用不同Docker网络模型搭建Zookeeper集群](https://github.com/denverdino/aliyungo/wiki/Zookeeper-cluster-with-Docker)

#### 命令

**客户端获取zookeeper服务性能及详细信息的四字命令**

用户在客户端可以通过 telnet 或 nc 向 ZooKeeper 提交相应的命令。

- conf 输出相关服务配置的详细信息。
- cons 列出所有连接到服务器的客户端的完全的连接 / 会话的详细信息。包括“接受 / 发送”的包数量、会话 id 、操作延迟、最后的操作执行等等信息。
- dump 列出未经处理的会话和临时节点。
- envi 输出关于服务环境的详细信息（区别于 conf 命令）。
- reqs 列出未经处理的请求
- ruok 测试服务是否处于正确状态。如果确实如此，那么服务返回“imok ”，否则不做任何相应。
- stat 输出关于性能和连接的客户端的列表。
- wchs 列出服务器 watch 的详细信息。
- wchc 通过 session 列出服务器 watch 的详细信息，它的输出是一个与watch 相关的会话的列表。
- wchp 通过路径列出服务器 watch 的详细信息。它输出一个与 session相关的路径。

**zkCli.sh 客户端连接服务端命令操作**

一般情况下zookeeper提供了三种方式：java/C客户端及zookeeper api进行交互，当然还有命令行的操作方式直接与zookeeper服务进行交互。

```
ZooKeeper -server host:port cmd args
	addauth scheme auth
	close 
	config [-c] [-w] [-s]
	connect host:port
	create [-s] [-e] [-c] [-t ttl] path [data] [acl]
	delete [-v version] path
	deleteall path
	delquota [-n|-b] path
	get [-s] [-w] path
	getAcl [-s] path
	history 
	listquota path
	ls [-s] [-w] [-R] path
	ls2 path [watch]
	printwatches on|off
	quit 
	reconfig [-s] [-v version] [[-file path] | [-members serverID=host:port1:port2;port3[,...]*]] | [-add serverId=host:port1:port2;port3[,...]]* [-remove serverId[,...]*]
	redo cmdno
	removewatches path [-c|-d|-a] [-l]
	rmr path
	set [-s] [-v version] path data
	setAcl [-s] [-v version] [-R] path acl
	setquota -n|-b val path
	stat [-w] path
	sync path
```

[Zookeeper基础命令操作](https://blog.csdn.net/dandandeshangni/article/details/80558383)

**监控**

由于网络上提供的监控工具有各种安全问题，这里不建议使用网络上的监控软件，这里自己写脚本，使用zabbix进行监控或者直接使用jmx进行监控。这里只讲zabbix监控。

zabbix监控zookeeper通过shell脚本获取各个监控项的值，从而通过zabbix-agent发送给server。

### 参考

1. [实战 -- Zookeeper实现分布式锁](https://blog.csdn.net/wangaiheng/article/details/82259211)
2. [ZooKeeper分布式锁实现java例子，附完整可运行源代码](https://blog.csdn.net/liyiming2017/article/details/83786331)
3. [ZooKeeper原理与应用](https://www.jianshu.com/p/84ad63127cd1)