---
title: 微服务架构实战 阅读笔记
date: 2020-04-29 13:18:59
tags:
 - 分布式
 - 阅读笔记
categories:
 - 阅读笔记
 - 分布式
---

微服务：软件架构模式，对系统进行模板划分，强调模块松耦合

<!--more-->

![YEq1Et.png](https://s1.ax1x.com/2020/05/06/YEq1Et.png)



优点：

- 开发简单：每个服务完成独立的功能
- 技术栈灵活：可以选择不同的语言完成不同的服务
- 服务独立无依赖
- 独立按需扩展
- 可用性高
- 复杂应用解耦为下而众的服务：将耗时的应用解耦
- 各服务精而专
- 服务间通信通过API完成

缺点：

- 多服务增加运维的复杂度
- 系统部署之间会有依赖
- 服务之间通信成本
- 数据一致性
- 系统集成挑战
- 重复工作
- 性能监控
- 沟通成本成倍增加

单体架构与分布式微服务架构对比：

![YEXmgU.png](https://s1.ax1x.com/2020/05/06/YEXmgU.png)

单体应用改进

1. 按服务垂直划分解决单体容错性差
2. 服务与客户端使用API网关解决客户端调用复杂
3. 增加注册中心解决服务之间调用复杂
4. 应用服务的容错性，容错机制、告警机制+网关层的反向代理

微服务可扩展性：

- 性能：尽量使用具有并发性和异步性的组件，具有完成通知功能的工作队列要优于同步连接到数据库
- 可用性：CAP理论，BASE理论：推荐整体使用最终一致性
- 维护：软件和服务，尽量使用自动化平台的工具监控和更新应用程序
- 成本：开发、维护和运营成本

#### 常见微服务组件

- 服务注册：记录当前可用的微服务实例的网络信息数据库，是服务发现机制的主要核心，服务注册表包含查询API（获取可用实例）、管理API（注册、注销）
- 服务发现：服务调用方从服务注册中心找到自己需要调用的服务的地址，可以选择客户端发现，亦可以选择服务端服务发现
- 负载均衡：服务提供方一般以多实例的形式提供服务，负载均衡功能能够让服务调用方连接道合适的服务节点，并且节点选择的工作对服务调用方来说是透明。服务端、客户端负载均衡
- 服务网关：服务调用的唯一入口，可以实现用户鉴权、动态路由、灰度发布、A/B测试、负载限流等功能，网关也应考虑高可用
- 配置中心：见本地化配置信息注册到配置中心，方便配置的统一维护和管理，高可用
- API管理：API文档的统一管理，加入版本控制
- 集成框架：需要将各个组件以配置的形式将所有微服务组件集成到统一的界面框架下
- 分布式事务：保证数据一致性
- 调用链：记录完成一个业务逻辑时调用到的微服务，同时统计各个服务的调用次数
- 支撑平台：需要平台提供服务的部署、运维、监控，例如，储蓄集成、蓝绿发布、健康检查、性能健康等

#### 常用微服务框架

- 主流：Spring Cloud、Dubbo
- 新锐微服务：Istio(服务网格)

#### 微服务架构设计模式

- 聚合器微服务设计模式

  ![YVpvZD.png](https://s1.ax1x.com/2020/05/06/YVpvZD.png)

- 代理微服务设计模式

  ![YV93LT.png](https://s1.ax1x.com/2020/05/06/YV93LT.png)

- 链式微服务设计模式

  ![YV92Yd.png](https://s1.ax1x.com/2020/05/06/YV92Yd.png)

- 分支微服务设计模式：聚合器的扩展

  ![YV9T0S.png](https://s1.ax1x.com/2020/05/06/YV9T0S.png)

- 数据共享微服务设计模式：单体到微服务，两个服务之间存在强耦合关系，需要共享缓存或者数据库

  ![YVCNB8.png](https://s1.ax1x.com/2020/05/06/YVCNB8.png)

- 异步消息传递微服务设计模式：避免类似REST设计模式的同步阻塞，使用消息队列代替REST请求、响应

  ![YVCr3n.png](https://s1.ax1x.com/2020/05/06/YVCr3n.png)

### 微服务设计原则

#### 分层架构

- MVC:模型、视图、控制
- MVP
- MVVM

领域模型架构：

- Domain：核心实体Entity、值对象Value Object
- Service：领域服务Domain Service
- Repositories：实体与值对象的存储和查询逻辑

![YeuaUs.png](https://s1.ax1x.com/2020/05/07/YeuaUs.png)



#### 统一通信协议



- Web Service 
- REST

通信协议：

- GRPC
- BRPC

#### 单一职责

SRP：降低函数、模块的功能复杂度

#### 服务拆分

按照单一职责原则和康威定律，在业务域、团队和技术上平衡粒度

AKF扩展立方体

![YeK0dH.png](https://s1.ax1x.com/2020/05/07/YeK0dH.png)

- x 轴 : 水平复制,即在负载均衡服务器后增加多个 Web 服务器 。
- y 轴 : 功能分解,将不同职能的模块分成不同的服务。从 y 轴方向扩展,才能将巨型应用分解为一组不同的服务,例如,订单管理中心、商品信息管理中心、库存管理中心等。
- z 轴 : 对数据库的扩展,即分库分表(分库是将关系紧密的表放在一台数据库服务器上,分表是因为 一 张表的数据太多,需要将一张表的数据通过 Hash 放在不同的数据库服务器上) 。

![YeKfeg.png](https://s1.ax1x.com/2020/05/07/YeKfeg.png)

![YeKblV.png](https://s1.ax1x.com/2020/05/07/YeKblV.png)

#### 前后端分离

- 前后端技术分离,可以由各自的专家来对各自的领域进行优化,这样前端的用户体验优化效果会更好。
- 分离模式下,前后端交互界面更加清晰,就剩下了接口和模型,后端的接口简洁明了,更容易维护。
- 前端多渠道集成场景更容易实现,后端服务无须变更,采用统一的数据和模型,可以支撑前端的 Web UI/移动 App 等访 问 。

前后端分离意味着前后端之间使用 J SON 来交流,两个开发团队之间使用 API 作为契约进行交互 。 从此,后台选用的技术技不影响前台 。

前后端分离的核心: 后台提供数据 ,前端负责显示。

#### 版本控制

在团队的内部,尤其是 API 设计优先 的微服务架构中,接口的版本管理显得尤其重要。

如果对现有服务执行破坏性的 API 更改,应决定如何管理这些更改 :

- 该服务是否会处理 API 的所有版本?
- 是否会维护服务的独立版本,以支持 API 的每个版本?
- 服务是否仅支持 API 的最新版本 ,依 靠其他适应层来与旧 API 来回转换?

通常可通过3种方式处理阻ST 资源的版本控制 。

![YeMG7Q.png](https://s1.ax1x.com/2020/05/07/YeMG7Q.png)

#### 围绕业务构建

系统可以根据业务切分为不同的子系统,子系统又可以根据重要程度切分为核 心和 非核心子系统。切分的目的就是当出现问题时,保证核 心业务模块正常运行,不影响业务的正常操作。同时解决各个模块子系统间的祸合、维护及拓展性 。方便单独部署,确保当 一 个系统出现问题时,不会 出现连锁反应而导致整个系统瘫痪 。

各个系统间合理地使用消息队列,解决系统或模块 间的 异步通信,实现高可用、高性能的通信系统 。

#### 并发流量控制

大流量一般的衡量指标就是系统的 TPS (每秒事务量)和 QPS (每秒请求量〉。其实这个没有一个绝对的标准,一般都是根据机器的配置情况而定的,如果当前配置不能应对请求量,那么就可以视为大流量了。

一般的应对方案包括 :

- 缓存
  预先准备好数据,减少对数据库的请求。

- 降级
  如果不是核心链路,那么就把这个服务降级,保证主干畅通 。

- 限流
  在一定时间内把请求限制在一定范围内,保证系统不被冲垮,同时尽可能提升系统的吞吐量。 限流的方式有几种,最简单的就是使用计数器,在 一段时间内,进行计数,与阔值进行比较,到了时间 临 界点,将计数器清 0 。

  应对并发,很重要的一点就是区分 CPU 密集型和 1/0 密集型。

- CPU 密集型 ( CPU-bound)

  CPU 密集型也叫计算密集型,指 的是系统的硬盘、内存性能相对 CPU 要好很多 。 此时,系统运作大部分的状况是 CPU Loading 100%, CPU 要读/写 I/0 (硬盘/内存〉在很短的时间就可以完成,而 CPU 还有许多运算要处理, CPU Loading 很高 。

  CPU bound 的程序一般而言 CPU 占用率相当高。这可能是因为任务本身不太需要访问 1/0设备,也可能是因为程序是多线程实现,因 此屏蔽掉了等待 I/0 的时间 。

- I/0 密集型( I/0 bound )

  I/0 密集型指的是系统的 CPU 性能相对硬盘、内存要好很多。 此时,系统运作大部分的状况是 CPU 在等 1/0 (硬盘/内存)的读/写操作,此时 CPU Loading 并不 高。

  I/O bound 的程序 一般在达到性能极限时, CPU 占用率仍然较低。这可能是因为任务本身需要大量 1/0 操作,而 pipeline 做得不是很好,没有充分利用处理器能力。

#### CAP

Consistency (一致性 )、 Availabili可(可用性)和 Partition tolerance (分区容错性),三者不可兼得

![YeQk3q.png](https://s1.ax1x.com/2020/05/07/YeQk3q.png)

![YeQmbF.png](https://s1.ax1x.com/2020/05/07/YeQmbF.png)

#### EDA 事件驱动

EDA 是一种以事件为媒介,实现组件或服务之间最大松耦合的方式 。

事件驱动则是调用者和被调用者互相不知道对方,两者只和中间消息队列相合 。

在事件驱动的架构中,跨服务完成业务逻辑的一个关键点是每个服务自动更新数据库和发布事件, 也就是要 以 原子粒度更新数据库和发布事件 。

![YeQd5d.png](https://s1.ax1x.com/2020/05/07/YeQd5d.png)

#### CQRS

CQRS 是命令查询责任分离 。最终 一致性
CQRS 架构由于本身只是 一个读写分离的思想,实现方式多种多样 。

比如数据存储不分离,仅仅只是代码层面读写分离,也是 CQRS 的体现 : 数据存储的读写分离, C 端负责数据存储,
Q 端负责数据查询, Q 端的数据通过 C 端产生的 Event 来同步,这种也是 CQRS 架构的 一种实现。

很多时候,我们也会放弃数据的强一致性,而采用最终 一致性;从 CAP 定理的角度来说,就是放弃 一致性,选择可用性 。

![YeQqZF.png](https://s1.ax1x.com/2020/05/07/YeQqZF.png)

将 一个微服务分为命令端、查询端和事件处理器,这三个部分可以相互独立地部署。

- 命令端
  请求采取命令的形式,可以驱动对微服务所拥有的领域数据的状态更改 。
- 事件处理器
  事件处理器可通过很多有用的方式对新的领域事件做出响应 。一个领域事件可以生成多个
  事件,这些事件可以发送到其他微服务 。
- 查询端
  查询端将提供一个 REST API ,允许 HTTP 客户端读取从己处理事件生成的实体化视图 。

#### 基础设施自动化

基础设施应该包括提供给业务相关的应用所有基础保障的服务和设施,比如 :

- DNS/CDN;
- 防火墙/Load Balancer;
- 应用服务器、数据库(物理机/虚拟机):
- 日志、监控、报警服务 。

一个服务应当被独立部署,并且包含所有的依赖、环境等物理资源。

#### 数据一致性

![Yel9sK.png](https://s1.ax1x.com/2020/05/07/Yel9sK.png)

#### 设计模式

微服务的设计模式主要有以下几种:链式设计模式、聚合器设计模式、数据共享设计模式和异步消息控制模式。

- 链式设计模式
  链式设计模式是常见的 一种设计模式,用于微服务之间的调用,应用请求通过网关到达第一个微服务,微服务经过基础业务处理,发现不能满足要求,继续调用第二个服务,然后将多个服务的结果统一返回到请求中

  ![YelZRI.png](https://s1.ax1x.com/2020/05/07/YelZRI.png)

- 聚合器设计模式
  聚合器设计模式是将请求统一由网关路由到聚合器,聚合器向下路由到指定的微服务中获取结果,并且完成聚合。首页展现、分类搜索和个人中心等通常都使用这种设计。

  ![Yel1oQ.png](https://s1.ax1x.com/2020/05/07/Yel1oQ.png)

- 数据共享模式
  数据共享模式也是微服务设计模式的一种。应用通过网关调用多个微服务,微服务之间的数据共享通过同一个数据库,这样能够有效地减少请求次数,并且对于某些数据量小的情况非常适合

  ![YelaLT.png](https://s1.ax1x.com/2020/05/07/YelaLT.png)

- 异步消息设计模式
  异步消息设计模式乍看起来跟聚合器的设计方式很像,唯 一 的区别就是网关和微服务之间的通信是通过消息队列而不是通过聚合器的方式来进行的,采用的是异步交互的方式

  ![YelrFJ.png](https://s1.ax1x.com/2020/05/07/YelrFJ.png)

#### DevOps

DevOps 希望做到的是软件产品交付过程中 IT 工具链的 打通,使得各个团队减少时间损耗,更加高效地协同工作

![YelgQx.png](https://s1.ax1x.com/2020/05/07/YelgQx.png)

无尽头的可能性: DevOps 涵盖了代码、部署目标的发布和反馈等环节,闭合成一个完整的DevOps 能力闭环 。

![YelROK.png](https://s1.ax1x.com/2020/05/07/YelROK.png)

#### 无状态服务

无状态服务原则并不是说在微服务架构里就不允许存在状态,其表达的真实意思是要把有状态的业务服务改变为无状态的计算类服务,那么状态数据也就相应地迁移到对应的“有状态数据服务”中。

无状态服务( Stateless Service )**对单次请求的处理,不依赖其他请求**,也就是说,处理一次请求所需的全部信息,要么都包含在这个请求里,要么可以从外部获取到(比如数据库),服务器本身不存储任何信息 。 Server 要设计为无状态的 。

### Spring Boot

#### 四大神器

- 自动配置
  Spring Boot 的 自动配置功能可基于类路径检测自动为运行中的应用配置依赖关系,不需要提供额外的 XML 配置。
- Starters
  Spring Boot 可提供一系 列称为 POM Starters 的精细依赖关系。 Spring Boot 熟知如何配置这些依赖关系, 同时让组织能够扩展 Spring Boot 来配置自定义的依赖关系。
- Actuator
  Actuator 可提供运行状况检查和指标等生产就绪型功能 。这些功能通过 Spring Boot 应用内的 REST 终端提供,只需要简单的配置就可 以 实现强大 的监控和检查。
- 开发工具
  这些工具旨在缩短开发和测试周期,其中包括一个可在资源变更时触发浏览器刷新的嵌入式 LiveReload 服务器。这些工具还提供了应用自动重启功能,只要类路径上的文件发生更改,该功能即可启动。重启技术使用两种类加载器,未更改的分类(例如,来自第三方 JAR 的 类)被加载到基础类加载器,而开发中的分类则被加载到重启类加载器。当应用重启时,重启类加载器会被丢弃, 同时创 建一个新的类加载器。这种方法意 味着应用重启 的速度通常要 比“ 冷启动”的速度快得多,因为基础类加载器已准备就绪且己填充完毕,从而快速实现应用的热部署,对于简单的修改这种场景能够非常有效地提高效率。

本章主要是Spring Boot的基本使用

### Docker

容器的实现之一

Docker 的目标 :

- 提供轻量简单的建模方式 ;
- 职责的逻辑分离;
- 快速高效的开发生命周期:
- 鼓励使用面向服务 的 架构 ,即 单个容器运行单个应用。

Docker 依赖的 Linux 的内 核特性包括 Name spaces 命名空间和 Control groups(c groups)控制组。

Namespaces 命名空间:

- PID ( process ID ) ,进程 ID 隔离 ;
- NET ( network ),管理 网络端口:
- IPC ( Inter Process Communication ),进程间通信;
- 管理跨进程通信的访问 :
- MNT ( Mount ),管理挂载点 ;
- UTS (UNIX Timesharing System ) ,隔离内核和版本标识:
- Control groups(c groups) ,控制组。

Control groups(c groups)控制组 :

- 资源限制;
- 优先级设定:
- 资源度量 :
- 资源控制及资源分配 ;

Docker 容器 的 能力包括 :

- 文件系统隔离一一每个容器都有自己的 root 文件系统,可以独立挂载外部文件系统。
- 进程隔离一一每个容器都运行在自己的进程环境中,相互之间互不干扰。
- 网络隔离 一一容器间的虚拟网络接口和IP地址都是分开 的 。
- 资源隔离和分组

Docker的安装、使用、Dockerfile、网络模式、数据卷

**Maven与docker结合**

1. xml

   ```xml
   <plugin>
       <groupid>com.spotify</groupid>
       <artifactid >dockerfile-maven-plugin</artifactId>
       <version>l.3.4</version>
       <configuration>
       <repository >${docker.image.prefix}/${project.artifactid}</repository>
       </configuration>
   </plugin>
   ```

2. 在项目根目录添加 Dockerfile 文件

   ```dockerfile
   FROM openjdk:8-jdk-alpine
   MAINTAINER hyp
   LABEL app="test-web" version="1.0.0" by="hyp"
   COPY ./target/test-web-1.0.0.jar app.jar
   EXPOSE 8080
   ENTRYPOINT ["java","-Djava.security.egd=file:/dev/./urandom -Xms10m -Xmx128m","-jar","/app.jar"]
   ```

3. 构建 Docker 镜像 

   ```
   mvn install dockerfile:build
   ```

### Spring Cloud

简化分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、断路器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署

Spring Cloud 下的“ Spring Cloud Netflix " 模块主要封装了 Netflix的以下项目。

- Eureka :基于阻 ST 服务的分布式中间件,主要用于服务管理。
- Hystrix : 容错框架,通过添加延迟阔值及容错的逻辑来帮助我们 控制分布式系统间组件的交互。
- Feign :一个 REST 客户端,目的是简化 Web Servic巳 客户端的开发 。
- Ribbon : 负载均衡框架,在微服务集群中为各个客户端的通信提供支持,它主要实现中间层应用程序的负载均衡 。
- Zuul :为微服务集群提供过代理、过滤 、 路由等功能。

除了 Spring Cloud Netflix 模块, Spring C loud 还包括以下几个重要的模块。

- Spring Cloud Config :为分布式系统提供了配置服务器和配置客户端,通过对它们的配
  置,可以很好地管理集群 中的配置文件。
- Spring Cloud Sleuth :服务跟踪框架 ,可以 与 Zipkin 、 Apache HTrace 和 ELK 等数据分
  析、服务跟踪系统进行整合,为服务跟踪 、 解决问题提供了便利 。
- Spring Cloud S仕earn : 用于构建消息驱动微服务的框架,该框架在 Spring Boot 的基础
  上 , 整合了“ Spring Integration ,, 来连接消息、代理中 间 件。
- Spring Cloud Bus :连接 RabbitMQ 、 Kafka 等消息代理的集群消息总线。

Spring Cloud 抛弃了 Dubbo 的盯C 通信,采用的是基于 HTTP 的阻ST 方式,这两种方式各有优劣。虽然牺牲了服务调用的性能,但也避免了上面提到的原生 RPC 带来的问题。而且REST 相比 盯C 更为灵活,服务提供方和调用方的依赖只依靠一纸契约,不存在代码级别的强依赖,这在强调快速演化的微服务环境下显得更加合适 。

![YnQfGd.png](https://s1.ax1x.com/2020/05/08/YnQfGd.png)

各组件的 运行流程如下 :

- 所有请求都统一通过 API 网关( Zuul) 来访问内部服务。
- 网关接收到请求后 ,从注册 中 心 (Eureka )获取可用服务。
- 由Ribbon 进行均衡负载后 , 分发 到后 端 的 具体实例 。
- 微服务之间通过Feign进行通信处理业务。
- Hystrix负责处理服务超时熔断。
- Turbine 监控服务间的调用和熔断相关指标。

后面讲述SC各模块的使用

### 自动化测试与质量管理

微服务在带来好处的同时,同样也带来了挑战, 具体可以总结为以下几点 。
- 系统依赖性增加

  由单体应用过渡到微服务架构时, 需要加入更多的隔离组件,这些组件的加入势必会带来更多的依赖性 。 也就是说,复杂性会变得更高 。 这同样给测试带来了相应的复杂度,原本只 需要从接口层开始的测试, 一 下子从深度上 多 出来更多的依赖,测试的工作量也是成倍地增加 。

  我们 拿一 个单体应用有 10 个接口的 工程举例,原本测试只 需 要测试这 10 个接口就可以,但是使用微服务构建之后,接口的数量 明显增加,每一个接口所衍生出来的新接口都需要测试,这样可能是一个倍数的关系 。 所以,微服务的接口抽象及经验就显得非常重要 。

- 并行开发
  接口数量的增加还会带来一 个问题,就是原本一 个团队维护的项目,被拆分之后,可能是两个或者更多的团队来进行维护 。 在需求变化时, 一 个接口的变动,导致的可能是连锁反应,当其中的 一 个接口出现延期时,整个测试计划就很难得到保证 。 团队需要等待其他团队以完整相关微服务的并行开发,微服务数量越 多 ,需要考虑的对象就越广泛,这意味着以并行方式开发及发布新功能就变得更加困难 。 所以,微服务的并行开发同样带来了管理上的挑战 。

- 与传统测试的冲突
  对于单块应用,在一个机器上就可以模拟出所有的依赖,但是在微服务场景下,由于依赖的服务往往很多,因此要 搭建一个完整的环境非常困难 。

- 更多潜在的故障
  微服务迁移的另 一 大负面影 响在于引发大量独立故障点 。

- 更多的团队交互
  微服务的方式对于测试来说,意味着 需要交互和沟通的人数必然增加,因为 一般的团队很难做到需求和实现的高效沟通。 沟通成本的增加有可能导致某个重要的功能点没得到优先测试,而不重要的功能点却被优先测试,从而导致整体工期的拖延 。

![Yn1vD0.png](https://s1.ax1x.com/2020/05/08/Yn1vD0.png)

- UI/UE 测试

  功能测试、兼容性测试、安装卸载测试等

- 端对端测试

- API 测试

  API 测试是针对系统所提供的 API 做各方面的验证。

- 组件接口测试

  尽管单独测试模块非常重要,但测试各个模块能否正确交互,并测试其作为子系统的交互性以查看接口的缺陷同样重要,这项工作可以通过集成测试来完成

- 单元测试

  单元测试的范围局限在服务内部,它是围绕着一组相关联的案例编写的。

P3C、FindBugs、SonarQube

**SonarQube**

```
docker pull sonarqube:community
docker run -d --name sonartest -p 8079:9000 sonarqube:community
```



### JHipster

代码生成器是按照特定编码规范输出代码的软件,可以直接生成项目,也可以单页生成,自从编程开始就有很多人进行过各种尝试。国内 也有很多这种快速开发的框架,但是这些框架除了个别的开源项目,往往持续性不够 , 不能够及时跟上技术发展的步伐 。 而且,对于代码的质 量 也没有进行很好的监控,没有一套完整的方案。那么,在微服务大行其道的当下,是不是有一款适合构建微服务的代码生成框架呢?答案就是 JHipster 。

- https://start.jhipster.tech/#/
- https://www.jhipster.tech/cn/

### 自动化部署

#### 私有仓库搭建

私服是架设在局域网的 一种特殊的远程仓库,目的是代理远程仓库及部署第三方构件。有了私服之后,当 Maven 需要下载构件时,直接请求私服,私服上存在则下载到本地仓库:否则,私服请求外部的远程仓库,将构件下载到私服,再提供给本地仓库下载 。

[Nexus](http://www.sonatype.org/nexus/go)

```
docker pull sonatype/nexus3
docker volume create nexus3_data
docker run -p 8081:8081 --name nexus3  -v  nexus3_data:/nexus-data -d  sonatype/nexus3
```

#### [Ansible](https://docs.ansible.com)

自动化运维工具

```shell
sudo yum install ansible 或者dnf install ansible
sudo apt install ansible

pip install --user ansible
#或者全局性安装
sudo pip install ansible
```

#### 持续集成

持续集成( Continuous integration ,简称 CD 是一种软件开发实践,即团队开发成员经常集成它们的工作,通常每个成员每天至少集成一次,也就意味着每天可能会发生多次集成。每次集成都通过自动化的构建(包括编译、发布、自动化测试〉来验证,从而尽快地发现集成错误。许多团队发现这个过程可以大大减少集成的问题,让团队能够更快地开发内聚的软件。

持续集成的优点:

- 快速发现错误。每完成一点更新,就集成到主干,可以快速发现错误,定位错误也比较容易。
- 防 止 分支大幅偏离主干。 如 果不是经常集成,主干又在不断更新, 则 会导致以后集成的难度变大,甚至难以集成 。

持续集成的一些原则 :

- 所有的开发人员需要在本地机器上做本地构建,然后提交到版本控制库中 , 从而确保他们的 变更不会导致持续集成失败。
- 开发人员每天至少 向 版本控制库中提交一次代码。
- 开发人员每天至少需要从版本控制库中更新 一次代码到本地机器。
- 需要有专 门 的集成服务器来执行集成构建,每天要执行多次构建。
- 每次构建都要 100%通过。
- 每次构建都可 以 生成可发布的产品。
- 修复失败的构建是优先级最高的事情。

持续集成工具如下。

- Apache Continuum:
- CruiseControl:
- Hudson:
- [Jenkins](https://www.jenkins.io/zh/): 
- Luntbuild :

选择Jenkins：

```
docker pull jenkins/jenkins:lts-alpine
docker volume create jenkins_data
docker run -d --name jenkins -p 8082:8080 -p 50000:50000 -v jenkins_data:/var/jenkins_home jenkins/jenkins:lts-alpine
```

### 日志收集与监控

![YnBJLF.png](https://s1.ax1x.com/2020/05/08/YnBJLF.png)

从左至右,先是从宿主服务器上去收集日志,紧接着是一个 管道 ,负责数据传输,接着数据被分流,一部分进了存储系统,比如 Elasticsearch 、 Hadoop ,一部分进了实时分析系统,比如 Storm 、 Spark Streaming 。这个架构已经被很多互联网公司采用。但这个架构设计有 几个地方值得大家关注 :

- 管道除了传输,还有很多能力,最常用的是简单预处理和数据缓冲。
- 传输可以是多级, 一般来说规模大到 一定程度 时,数据往一个点的海量汇聚很容易出问题,分级处理是个很好的方式 。
- 日志收集方式尽量统一,很多设计里对于业务日志、容器日志(如 Docker )、宿主机日志等采用不同 agent ,这是一个不好 的设计方式 , agent 也 要管理 ,引入太多 agent只会给自 己增加不必要的麻烦。

现在,组件托管在位于私有云、公共云或两者的混合体之间的虚拟化机器或容器内。并不需要关 心服务 CPU 用了多少,内存用了 多少。 只需要确保这些服务相互通信以提供所需的结果即可。需要从监控的角度看 几件事情 :

- 微服务集群中是否所有的服务的吞吐率 、响 应时间都正常?
- 服务调用线中哪些线负载过大 ,哪些线负 载过小?
- 服务的错误率,如 HTTP 500 错误。

#### ELK 搜集与分析

ELK 是 Elasticsearch 、 Logstash 和 Kibana 的 简称,这三套开源工具组合起来能搭建一套强大的集中式日志管理平台。

- Elasticsearch 是一个开源的分布式搜索引擎,提供搜索、分析、存储数据三大功能 。 它的特点有 : 分布式、自动发现、和 | 自动分片 、 索引 副 本机制、阻STful 风格接口、多数据源及自动搜索负载等。
- Logstash 是一个开源的用来收集、解析、过滤日志的工具 , 支持几乎任何类型 的日志,包括系统日志、业务日志和安全日志 。它 可以从许多来源接收日志,这些来源主要包括 Syslog 、消息传递(例如, RabbitMQ )和 
- Filebeat ;能够 以 多种方式输出数据,这些方式主要包括电子邮件、 Web Sockets 和 Elasticsearch 。
  Kibana 是一个基于 Web 的 友好图形界面 ,用于搜索 、 分析和可视化存储在 Elasticsearch 中的 数据。它利用 Elasticsearch 的 RESTful 接口来检索数据,不仅允许用户定制仪表板视图,还允许他们以特殊的方式查询、汇总和过滤数据。

在需要收集日志的所有服务上部署 logstash , 作为 logstash agent ( logstash shipper )用于监控并过滤收集日志,将过滤后的内容发送到 Redis , 然后 logstash indexer 将日 志收集在一起交给全文搜索服务 Elasticsearch ,可以 用 Elasticsearch 进行自定义搜索,通过 Kibana 结合自定义搜索进行页面展示。 Logstash 收集 App Server 产生 的 Log , 并存放到 El asticsearch 集群中 ,而 Kibana则从 ES 集群 中 查询数据生成图表 ,再返回给 Browser 。

日志收集主要分为访问日志和业务日志两部分。

**部署**

开启Linux系统Rsyslog服务：

修改Rsyslog服务配置文件：

```undefined
vim /etc/rsyslog.conf
```

开启下面三个参数：

```ruby
$ModLoad imtcp
$InputTCPServerRun 514

*.* @@localhost:4560
```

意图很简单：让Rsyslog加载imtcp模块并监听514端口，然后将Rsyslog中收集的数据转发到本地4560端口！

然后重启Rsyslog服务：

```undefined
systemctl restart rsyslog
```

查看rsyslog启动状态：

```
netstat -tnl
```



```shell
docker pull elasticsearch:latest
docker pull kibana:latest
docker pull logstash:latest

#启动Elasticsearch
docker volume create elasticsearch_data
docker run --name elasticsearch \
-v elasticsearch_data:/usr/share/elasticsearch/data \
-p 9200:9200 \
-d elasticsearch

#启动Kibana
docker run --name kibana \
--link elasticsearch:elasticsearch \
-e ELASTICSEARCH_URL=http://elasticsearch:9200 \
-p 5601:5601 \
-d kibana
#容器启动后用浏览器访问5601端口

#启动Logstash
#以nginx的访问日志为例，我们配置logstash读取nginx的access.log，然后把日志转发到Elasticsearch
```

logstash.conf

```
input {
  syslog {
    type => "rsyslog"
    port => 4560
  }
}

output {
  elasticsearch {
    hosts => [ "elasticsearch:9200" ]
  }
}
```

启动容器，这里我们把nginx的日志放在/tmp/nginx/logs/access.log，为了让容器能读到这个日志，需要把日志目录映射到容器里面。

```
docker run -d -p 4560:4560 \
-v ~/logstash/logstash.conf:/etc/logstash.conf \
--link elasticsearch:elasticsearch \
--name logstash logstash \
logstash -f /etc/logstash.conf
```

启动nginx容器来生产日志

```
docker run -d -p 90:80 --log-driver syslog --log-opt \
syslog-address=tcp://localhost:514 \
--log-opt tag="nginx" --name nginx nginx
```

**监控工具**

- Open-Falcon
- Zabbix

**APM 监控**

APM = Application Performance Management ,应用性能 管理,对企业系统即时监控以实现对应用程序性能管理和故障管理的系统化的解决方案。

- [Pinpoint](http://naver.github.io/pinpoint/)
- SkyWalking
- Zipkin
- CAT

### 微服务核心功能推荐

#### 工作流引擎

作流( workflow )就是工作流程的计算模型,即将工作流程中的工作如何前后组织在一起的逻辑和规则在计算机中以恰当的模型进行表示并对其实施计算 。 它 主要解决 的是“使在 多 个参与者之间按照某种预定义的规则传递文档、信息或任务的过程自动进行,从而实现某个预期的业务目标,或者促使此目标的实现”。

- [Activiti](https://www.activiti.org)
- [UFLO](https://gitee.com/mirrors/uflo)
- Drools
- URule

#### 调度系统

[XXL-JOB](https://github.com/xuxueli/xxl-job) 是一个轻量级分布式任务调度框架,其核心设计目标是开发迅速、学习简单、轻量级、易扩展。现己开放源代码并接入多家公司线上产品线,开箱即用 。

#### 消息推送

[MPUSH](https://github.com/mpusher/mpush) 是一款开源的实 时消息推送系统,采用 Java 语 言 开发,服务端采用模块化设计,具有协议简洁、传输安全、接口流畅、实时高效、扩展性强、可配置化、部署方便、监控完善等特点,同时也是少有的可商用的开源推送系统。

#### 网关申闰件

常用的网关中间件有 Zuul、Kong、Tyk 和 Orange 等。

#### 分库分表申闰件

- Sharding-JDBC
- MyCat

#### 报表引擎

[UReport2](https://github.com/youseries/ureport) 是一款基于架构在 Spring 之上的纯 Java 的高性能报表引擎,通过迭代单元格可以实现任意复杂的中国式报表 。 相比 UReport1, UReport2 重写了全部代码,弥补了 UReport1在功能及性能上的各种不足 。

#### 数据处理

ETL 中三个字母分别代表的是 Extract、 Transform、 Load ,即抽取、转换、加载 。

- 数据抽 取 : 从源数据源系统抽取目的数据源系统需 要 的数据。
- 数据转换 : 将从源数据源获取的数据按照业务需求,转换成目的数据源要求的形式,并对错误、不一致的数据进行清洗和加工。
- 数据加载 : 将转换后的数据装载到目的数据源 。

ETL 原本是作为构建数据仓库的 一个环节 ,负 责将分布的、异构数据源中的数据(如关系数据、平面数据文件等)抽取到临时中间层后进行清洗、转换、集成,最后加载到数据仓库或数据集市中,成为联机分析处理、数据挖掘的基础 。 现在也越来越多地将 ETL 应用于 一般信息系统中数据的迁移、交换和同步。

- Spring Batch
- Kettle

#### 并发编程

[Akka](https://akka.io) 是 NM 平台上构 建高并发、分布式和容错应用的工具包和运行 时 。 Akka 用 Scala 语言写 成,同时提供了 Scala 和 Java 的开发接口。

使用 Akka 框架编 写 的应用 程序既可 以横 向扩 展( Scale Out ) ,也可纵向扩展 ( Scale Up ) 。

#### 分布式配置

在 Spring Cloud 中, 由 Spring Cloud config 实现分布式配置,其实有 已经集成好的分布式配置的项目可供我们选择 。

- Disconf
- Apollo

#### CAS

CAS 框架 : CAS ( Central Authentication Service ) 是实现 s so 单点登录的框架。

从结构上看, CAS 包含两个部分: CAS Server 和 CAS Client 。 CAS Server 需要独立部署 ,主要负责对用户 的认证工作; CAS Client 负 责处理对客户端受保护资源的访 问请求, 需要登录时,重定向到 CAS Server 。

CAS Client 与 受保护 的客户端应用部署在一起,以 Filter 方式保护 Web 应用的受保护资源,过滤从客户端过来的每一个 Web 请求,同时, CAS Client 会分析 HTTP 请求中是否包请求 Service Ticket ,如果没有,则说明 该用户是没有经过认证的,于是 CAS Client 会重定向用户请求到 CAS Server。

然后是用户认证过程,如果用户提供了正确的 Credentials, CAS Server 会产生 一个随机的Service Ticket 。缓存该 Ticket ,并且重定向用户到 CAS Client (附带刚才产生的 Service Ticket ),Service Ticket 是不可以伪造的 。最后 CAS Client 和 CAS Server 之间完成了 一 个对用户的身份核实 ,用 Ticket 查到 Username o 该协议完成了 一个很简单的任务,所有与 CAS 的交互均采用 SSL协议,确保 ST 和 TGC 的安全性 。 协议工作过程会有 2 次重定向过程,但是 CAS Client 与 CAS Server 之间进行 Ticket 验证的过程对于用户是透明的。

CAS 认证的协议包括 CAS 、 OA.uth 、 OpenID 、 SAML 、 REST 。

#### WebFlux

反应式编程